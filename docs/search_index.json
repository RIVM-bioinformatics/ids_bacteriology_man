[["index.html", "Bioinformatics Pipelines for Bacteriology (IDS, RIVM) 1 Introduction and General Instructions 1.1 General Instructions for all pipelines", " Bioinformatics Pipelines for Bacteriology (IDS, RIVM) Alejandra Hernandez-Segura, Roxanne Wolthuis 2021-09-10 1 Introduction and General Instructions In this guide you will find the manuals of all the available pipelines for the Bacteriology and Parasitology Department of the IDS (RIVM). This guide is in continuous development, For additions and adjustments to this guide contact Roxanne Wolthuis. Note: This guide was written exclusively for users working at the RIVM, in specific the IDS department. If you are an external user it is necessary to adjust the parameters or scripts from the pipelines. Detailed information about the pipelines and parameters can be found in the repositories of each pipeline. All developed pipelines are available at the RIVM Github. Please keep in mind that the main goal for our pipelines is to automate processes for the RIVM. This means we cannot give the personalized and fast-responding help that we give to users from the RIVM. If you have a question or need help please write an Issue in the corresponding GitHub repository and we will try to help as soon as possible. 1.1 General Instructions for all pipelines 1.1.1 Requirements and preparation This guide is written based on the Wetenschappelijke werkplek at the RIVM. It is possible to run the Juno pipelines on your laptop, but this requires additional steps that are not listed in this guide. 1. Placement of the data All data should be placed in one folder within the Biogrid folder (/data/BioGrid/your_folder/your_data/)or scratch_dir folder (/mnt/scratch_dir/your_folder/your_data). Sub folders are not allowed. It is advised to store your data in the scratch_dir folder, this will make the analysis faster. After analysis it is possible to copy the output to the Biogrid folder. 2. Naming restrictions on the folders In order to successfully run the Juno pipelines it is necessary to use the correct file names for all the folders in your input path. The input path is the path that leads to your input directory. For example /mnt/scratch_dir/your_folder/your_data. All folder names inside this path can only contain letters, numbers or underscores. Using spaces, different characters or special symbols inside your folder will cancel the analysis, because the Juno pipelines cannot recognize these kinds of folder names. 3. File format The files in the input need to have the right format. If you have .fastq files the extensions can be: .fastq, .fq, .fastq.gz or .fq.gz. If you have .fasta files the extension can be: .fasta. In some cases it is possible to use both .fasta and .fastq files as input. 4. Naming restrictions on the files Not only the folder names, but also the names of the input files have some restrictions. The file names can contain letters, numbers and underscores. If you are using .fastq files, the file name always has to end with _R1, _R2, _1 or _2. If you use these ends in other places in the file name the pipeline will crash. Below is a good and a bad example of file names. If you use both file formats as input it is important that the files have identical names(except for the part that indicates the read of the .fastq file(R1 and R2). Good file name: IBESS11_S13_L001_R1.fastq.gz Bad file name: IBESS11_1_S13_L001_R1.fastq.gz Example of file names by using both .fastq and .fasta as input: IBESS11_S13_L001_R1.fastq.gz IBESS11_S13_L001_R2.fastq.gz IBESS11_S13_L001.fasta 1.1.2 Downloading pipelines All Juno pipelines are stored in Github or the internal Gitlab of the RIVM. The Juno pipelines inside GitHub are publicly accessible. The Juno pipelines in the internal Gitlab can only be accessed by users that are inside of the servers or environments of the RIVM. You can enter the internal Gitlab with your RIVM log in details. Note: If you download one of the Juno pipelines make sure to download it into the same partition to where your data is stored, preferentially scratch_dir. The instruction on where to retrieve the Juno pipeline can be found on the documentation page of each of the pipelines. 1.1.2.1 Download through Github/Gitlab website Go to the documentation page of the pipeline Click the Github or Gitlab link For Github  Click the green Code button on the page and click on Download zip(Figure 1.1 step 2) For Gitlab  Click the white Download button click on Download zip(Figure 1.3 step 2) The zip file will now be in your Downloads folder, move the zip file to BioGrid partition or the scratch_dir partition. The download should be placed in the same partition as where your data is stored Extract the files of the zip file. In the Wetenschappelijke Werkplek this can be done by right clicking the downloaded zip file, then Open with Archive Manager and then press Extract on the two windows that will consecutively appear (Figure 1.2). FIGURE 1.1: Step 1 shows where to retrieve the Github clone link. Step 2 shows where to retrieve the .zip file. FIGURE 1.2: Step 1 shows where to retrieve the Gitlab clone link. Step 2 shows where to retrieve the .zip file. FIGURE 1.3: Unzipping a repository in the Wetenschappelijke Werkplek. 1.1.2.2 Download through command line In the Wetenschappelijke Werkplek go to Applications &gt; System Tools &gt; Open the Terminal(Figure 1.4) In the terminal, use the cd command to navigate towards the location where you want to download the pipeline cd /mnt/scratch_dir/your_folder/ Get the url of the pipeline For Github  Click on the green Code button and copy the link (Figure 1.1 step 1) For Gitlab  Click on the blue Clone button and copy the link (Figure 1.3 step 1) Use the git clone command to download the pipeline git clone https://github.com/link_to_clone.git The pipeline will ask for your username and password, if you have access and the log in details are correctly entered the pipeline will be downloaded and unzipped in the folder you used. Note: Every git url that is used to clone a repository always ends with .git FIGURE 1.4: Opening a terminal inside the Wetenschappelijke Werkplek. 1.1.3 What to expect while running a Juno pipeline The Juno pipelines usually run automated. Minimum input of the user is required. Detailed information on the required input can be found in the section for the specific pipeline. Sometimes a pipeline can ask for input from the user to agree on installing software or a database (Figure 1.5 step 1), if you get asked to give permission, please do so in order to run the pipeline. This is most often in the form of pressing y followed by enter (Figure 1.5 step 2). The first time a Juno pipeline is executed, the preparation might take longer than expected. This is due to the installation of the basic software that is required for the pipeline to run. Be patient! You can recognize that the pipeline is still preparing when it shows a blinking box (Figure 1.5 step 3) or when it keeps printing lines inside the terminal. If the installation takes longer than 1 hour check the section General Troubleshooting. FIGURE 1.5: Screenshot of the Juno-typing pipeline in the terminal. The pipeline asks to create an environment(step 1). User input is required. To proceed the pipeline press y followed by enter. If executed correctly the pipeline will now show that it is creating the environment(step 2). The blinking box at the bottom indicates that the pipeline is still installing/updating(step 3). After installation the Juno pipeline will start running. Now the terminal shows yellow and/or green messages. These messages indicate that the pipeline is being executed (Figure 1.6). At the start of the yellow messages the number of steps is shown (Figure 1.6 step 1). The amount of steps indicate the progress of the pipeline in %. Once the pipeline is finished there will be a message printed that all the steps are performed (Figure 1.6 step 2). FIGURE 1.6: Screenshot of the Juno AMR pipeline in the terminal. The pipeline is executing all required jobs to create the requested results. The terminal shows a list of all the jobs that will be executed(step 1). Furthermore there is green and yellow text that shows the steps of the pipeline. At the bottom of the terminal there is a line that shows the pipeline is finished(step 2). If a step fails, you will likely see red text appearing on the screen (Figure 1.7). The pipeline might proceed the other steps. If the pipeline fails check the section General Troubleshooting for more help or any other problems you may encounter. FIGURE 1.7: Screenshot of the Juno AMR pipeline in the terminal. The pipeline failed on one of the steps. The red text shows an error message. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max. With sequencing data, the storage space can get full quite fast. 1.1.4 General output and log files for every pipeline The output of the Juno pipelines is divided over multiple folders inside the output folder. For each pipeline, there is an audit_trail folder, log folder, and result folder for each step or tool. The folders with results are probably the most interesting and contain most of the information you would use for analysis. The audit_trail folder contains information about the software, parameters, samples, and more. This kind of information can be useful for reproducibility or publications. The log folder contains information that is required for debugging and/or troubleshooting. For the developers, it is essential to have access to these files when an error occurs. All the output will be shortly discussed below. An example of an output folder is shown in Figure 1.8. result folder(s) The output folder contains one result folder for each step that is performed by the pipeline. Inside the folder, there are results for each of the samples. The result output is different for each pipeline, you can check the pipelines page or the tool that is used for more information about the output. audit_trail folder The information in the audit_trail folder can be useful for the reproducibility of the pipeline and analysis. The log_conda.txt file contains information about the software that was used to run the pipeline. The log_config.txt enlists all parameters that were used to run the pipeline. The log_git.txt contains information about repositories or the code that was downloaded. The snakemake_report.html contains an overview of all the steps that were performed with the samples in the input folder. It shows when the steps were performed, what output was produced, what software was used as well as some statistics about the run. log folder The log/cluster/ or log/drmaa/ folder contains .error and .out(log) files. For each step in the pipeline, there is one .error and one .out file. These files are useful to debug system/memory errors. If a step fails to perform and an error occurs, the .error file will show information about the error. The .out file contains information that is normally shown on the command line. This information can be originating from the tools that are used or from the pipeline itself. Sometimes these files can be empty, this happens if there were no problems or messages generated on the run or because the problem lies before the job/analysis was even started. You will need this output if you want to contact a developer when you run into problems. You can send the files or the location(example: mnt/scratch-dir/mydirectory/output/log) of the files when you contact the developer. Note: Do not delete the contents of the log folder. FIGURE 1.8: Example of an output folder. 1 shows the audit_trail folder, 2 shows output folders per step or tool, 3 shows the log folder. "],["general-troubleshooting.html", "2 General Troubleshooting 2.1 Failure when installing master environment 2.2 Failure to make a sample sheet or to find input directory 2.3 Other problems or failing rules", " 2 General Troubleshooting body { text-align: justify} The Juno pipelines are fully automated and require minimum installation and input. We tried to optimize the pipelines as best as possible, but sometimes your analysis might fail. Below is a troubleshooting guide that contains the most common reasons why it could be that your analysis is failing, including a step by step overview on how to fix them. Note: This guide was written exclusively for users working at the RIVM, in specific the IDS department. If you are an external user it is necessary to adjust the parameters or scripts from the pipelines. Detailed information about the pipelines and parameters can be found in the repositories of each pipeline. All developed pipelines are available at the RIVM GitLab[link]. Please keep in mind that the main goal for our pipelines is to automate processes for the RIVM. This means we cannot give the personalized and fast-responding help that we give to users from the RIVM. If you have a question or need help please write an Issue in the corresponding GitHub repository and we will try to help as soon as possible. 2.1 Failure when installing master environment Settings within the Wetenschappelijke Werkplek can cause problems when installing one of the pipelines. Usually installation will take a few minutes, during installation a message will be printed to the command line: Solving environment. If this message is shown by the command line for 30 minutes or more, abort the step by pressing CTRL + Z or CTRL + C until the pipeline exits installation. After exiting the installation you can change the necessary settings. Once the settings are changed it is possible to install the master environment again. command to change settings inside the Wetenschappelijke Werkplek: conda config --set channel_priority false 2.2 Failure to make a sample sheet or to find input directory If the pipeline sends error messages about the sample sheet, input files or input directory it means that there is something wrong with the given input. Below is a list of suggestions for troubleshooting this problem. Your path (all the folders that lead to your input directory) contains spaces or unrecognized characters. You mis-spelled something in your path or gave a path that leads to the wrong input directory If you work in the Wetenschappelijke Werkplek it is required to place a (/) at the beginning of the fill path. For instance: /mnt/scratch_dir/&lt;your_folder&gt;/&lt;your_data&gt;/. The files might have the incorrect format and/or extension. Make sure that all the files in the input directory have the correct extension. An overview of the extensions that are allowed can be found in section: 1.1.1 Requirements and preparation The files in your input directory are in sub-folders. Most pipelines expect all input files to be in one folder instead of scattered in sub-folders. The only exception is when you use output from a previous pipeline (for instance, Juno-assembly) into another pipeline. 2.3 Other problems or failing rules The Juno pipelines are still in development which means that sometimes a process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of the guide. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: We cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "],["juno-assembly.html", "3 Juno-assembly pipeline 3.1 Handbook", " 3 Juno-assembly pipeline body { text-align: justify} The goal of this pipeline is to generate assemblies from raw fastq files. The input of the pipeline is raw Illumina paired-end data in the form of two fastq files (with extension .fastq, .fastq.gz, .fq or .fq.gz), containing the forward and the reversed reads (R1 and R2 must be part of the file name, respectively). On the basis of the generated genome assemblies, low quality and contaminated samples can be excluded for downstream analysis. Note: The pipeline has been tested only in gastroenteric bacteria ( Salmonella, Shigella, Listeria and STEC) but it could theoretically be used in other genera/species. The pipeline uses the following tools: FastQC (Andrews, 2010) is used to assess the quality of the raw Illumina reads Trimmomatic (Bolger, Lohse, &amp; Usadel, 2014) is used to remove poor quality data and adapter sequences. The sliding window option of Trimmomatic starts scanning at the 5 end and clips the read once the average quality within the window falls below a threshold. The sliding window is the number of nucleotides over which Trimmomatic calculates an average phred quality score, a measure of the quality of the identification of the nucleobases generated by automated DNA sequencing. The Trimmomatic minlen config parameter is set to 50, this parameter is used to drop the read if the read is below a specific length. FastQC is used once more to assess the quality of the trimmed reads Picard determines the library fragment lengths The reads are assembled into scaffolds by SPAdes (Bankevich et al., 2012) by means of de novo assembly of the genome. SPAdes uses k-mers for building an initial de Bruijn graph and on following stages it performs graph-theoretical operations to assemble the genome. Kmer sizes of 21, 33, 55, 77 and 99 were used. For de novo assembly, SPAdes isolate mode is used. QUAST (Gurevich, Saveliev, Vyahhi, &amp; Tesler, 2013) is used to assess the quality of the filtered scaffolds. To assess the quality of the microbial genomes, CheckM (Parks, Imelfort, Skennerton, Hugenholtz, &amp; Tyson, 2015) is used. CheckM calculates scores for completeness, contamination and strain heterogeneity. Bbtools (Bushnell, 2014) is used to generate scaffold alignment metrics. MultiQC (Ewels, Magnusson, Lundin, &amp; Käller, 2016) is used to summarize analysis results and quality assessments in a single report for dynamic visualization. 3.1 Handbook 3.1.1 Requirements and preparation See the General Instructions for all pipelines first. You have different possibilities to run the complete pipeline. Your input directory should contain samples with just ONE genus type. If this is the case, you should tell the pipeline which genus you have (e.g. using the flag genus Salmonella). Read further for more details. You have to make sure that your sample was listed in the Excel file: /data/BioGrid/NGSlab/BAC_in_house_NGS/In-house_NGS_selectie_2021.xlsx (look at the first sheet) and that a genus was provided in the appropriate column. If this is not the case DO NOT MODIFY THAT EXCEL FILE! Instead follow the steps mentioned in the Troubleshooting section, particularly in the part Error saying that my samples cannot be found on the Excel file with the genus list You could skip the step that uses the CheckM tool. That step is the only one that actually requires the information about the genus. You would be missing the statistics about how complete the assembly is with respect to the reference genome. 3.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-assembly pipeline can be found in this link. 3.1.3 Start the analysis. Basics Open a terminal. (Applications &gt; terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_pipeline Run the pipeline If all your samples have the same genus, for instance, Salmonella, you run it like this: bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --genus Salmonella Note that the genus should be ONE word. Do not put any species names! If your samples do not have all the same genus but they are present in the file /data/BioGrid/NGSlab/BAC_in_house_NGS/In-house_NGS_selectie_2021.xlsx, then do: bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ If you do not manage to use any of these steps or you prefer to skip the step with CheckM (this step calculates genome completeness and gives a proxy of contamination) then do: bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --no-genus Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 3.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per result (qc_raw_fastq, clean_fastq, qc_clean_fastq, de_novo_assembly, de_novo_assembly_filtered, qc_de_novo_assembly). Please refer to the manuals of every tool to interpret the results. . Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 3.1.5 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 3.1.5.1 Error saying that the genus supplied was not recognized by CheckM If you get this message: ERROR: The genus supplied with the sample(s): Sample_name1 Sample_name2 were not recognized by CheckM Please supply the sample row in the Excel file /data/BioGrid/NGSlab/BAC_in_house_NGS/In-house_NGS_selectie_2021.xlsx with a correct genus. If you are unsure what genera are accepted by the current version of the pipeline, please run the pipeline using the --help-genera command to see available genera. it means that although you provided the genus information, it is probably not supported by the tool CheckM. First check that you have no spelling mistakes in the genus name of your sample(s) or that you used the correct case (first letter capital and the rest on small letters). If you are sure that your spelling is correct, it may be that CheckM does not have the genus you supplied on its database. As the message says, if you are not sure on how to spell the genus name or which genus is accepted, please check the list of accepted genera by typing. bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --help-genera 3.1.5.2 Other problems or failing rules The Juno-assembly pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "],["juno-typing.html", "4 Juno-typing 4.1 Handbook", " 4 Juno-typing body { text-align: justify} The goal of this pipeline is to perform bacterial typing (7-locus MLST and serotyping). It takes 2 types of files per sample as input: Two .fastq files (paired-end sequencing) derived from short-read sequencing. They should be already filtered and trimmed (for instance, with the Juno-pipeline). An assembly from the same sample in the form of a single .fasta file. Importantly, the Juno-typing pipeline works directly on output generated from the Juno-assembly pipeline. The Juno-typing pipeline will then perform the following steps: Recognizing the genus/species by using the KmerFinder tool Once the genus and species is recognized, they are used to choose the appropriate 7-locus MLST schema and eventually a serotyper. The supported species for the 7-locus MLST can be found in the database generated by the Center for Genomic Epidemiology from the Technical University of Denmark 7-locus MLST by using the MLST tool If appropriate for the genus/species, the samples will be serotyped. The currently supported species are: Salmonella serotyper by using the SeqSero2 tool and an in silico adaptation of the PCR suggested by Tennant et al. 2010 to differentiate monophasic from biphasic S. typhimurium. E. coli serotyper by using the SerotypeFinder tool. S. pneumoniae serotyper by using the Seroba tool. Shigella serotyper by using the ShigaTyper tool. Disclaimer!! Importantly, the genus and species are automatically detected from the processed reads (using kmerFinder) and the schema to use for the 7-locus MLST is chosen accordingly. The results of kmerfinder are also included in the output. However, this pipeline is NOT meant for bacteria identification and it has not been tested for it. Even if similar species would be misidentified, they would probably share the same MLST7 scheme. Following that thought, the correct choice of scheme has been tested, but not the correct species identification. 4.1 Handbook 4.1.1 Requirements and preparation See the General Instructions for all pipelines first. This pipeline needs two fastq file (R1 and R2) and an assembly (.fasta) files per sample. The fastq files should have been trimmed and filtered to remove low quality reads/bases. You could use the Juno-assembly pipeline for that. Moreover, that pipeline also provides the de novo assembly for your samples and the output folder of the Juno-assembly pipeline can be used directly into the Juno-typing pipeline. If you, however, prefer to use any other tool for doing your assembly and trimming/filtering, make sure that the fastq files and fasta files have the same name (for instance, sample1_R1_001.fastq.gz, sample1_R1_001.fastq.gz and sample1.fasta). If that is not the case, the files may not be recognized as belonging to the same sample. Also, ALL THREE FILES SHOULD BE IN THE SAME FOLDER! If you have multiple samples, they should all be in the same input folder, NOT IN SUBFOLDERS. The only exception is if you use the Juno-assembly pipeline to pre-process your data. In that case, the pipeline will recognize the subfolders where the fastq files and the fasta files should be. 4.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-typing pipeline can be found in this link. 4.1.3 Install conda environment YOU NEED TO REINSTALL THE MASTER ENVIRONMENT EVERY TIME YOU UPDATE THE PIPELINE (everytime you download the code) Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-typing If you already had a juno_blast environment before you need to delete the old one by using the command: conda env remove -n juno_typing If you had never created a juno_blast environment before, you can skip this step and go to step 4 instead. Create a new environment for running Juno_blast by using the command: conda env create -f envs/master_env.yaml This step will take some time (few minutes). Note: If this step would take more than 1 hour, please kill the process (using Ctrl + C or Ctrl + Z) and refer to the section General Troubleshooting. The first issue written there (Failure when installing master environment) often solves the problem. If, however, the problem persists, please contact me by email. 4.1.4 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-typing Activate juno_typing environment conda activate juno_typing Run the pipeline This can be done in two ways. The first one is just providing an input directory: python juno-typing -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ The second one is providing as well a metadata (csv) file. This file should contain at least one column with the Sample name (name of the file but removing [_S##]_R1.fastq.gz), a column called Genus and a column called Species. If a genus + species is provided for a sample, it will overwrite the species identification performed by this pipeline when choosing the scheme for MLST and the serotyper. Example metadata file: Sample Genus Species sample1 Salmonella enterica Note: The fastq files corresponding to this sample would probably be something like sample1_S1_R1_0001.fastq.gz and sample2_S1_R1_0001.fastq.gz and the fasta file sample1.fasta. Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 4.1.5 Output A folder called output/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (identify_species, mlst7 and serotype). Please refer to the manuals of every tool to interpret the results. In each one of these folders, there should be a sub-folder per sample and, for the case of mlst7 and serotype, also a .csv file collecting the results of all the samples together (serotype/serotype_multireport.csv, serotype/serotype_multireport1.csv, serotype/serotype_multireport2.csv, serotype/serotype_multireport3.csv, mlst7/mlst7_multireport.csv). Although the results of kmerfinder are provided (in the subfolder identify_species), these have not been validated. They are used only to choose the right scheme for the MLST and the right serotyper (unless you provided metadata). If you would use the results of kmerfinder as a species identification tool, you do it at your own risk and you should be able to interpret the results correctly. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash juno-typing -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later time point and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 4.1.6 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 4.1.6.1 Other problems or failing rules The Juno-typing pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed If the problem is related to Shigella or Shigatyper, please send an e-mail to Roxanne Wolthuis. Note: I cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "],["juno-amr.html", "5 Juno-amr 5.1 Handbook", " 5 Juno-amr body { text-align: justify} The Juno Antimicrobial Resistance(Juno AMR) pipeline is a pipeline that is used to automate multiple tools that help identify acquired genes and find chromosomal mutations mediating antimicrobial resistance in DNA sequences of bacteria. The tool takes paired-end .fastq files or .fasta files as input. The input files can contain full or partial sequences. The output can be used in antimicrobial resistance analysis and is combined in four summary files for a quick overview of the most important results. The tools that are being used in this pipeline are listed below: ResFinder is a tool that identifies acquired antimicrobial resistance genes based on databases. The tool is created by The Center For Genomic Epidemiology. PointFinder is a tool that searches for chromosomal point mutations that mediate resistance to select antimicrobial agents for some bacterial species. The tool is created by The Center For Genomic Epidemiology. 5.1 Handbook 5.1.1 Requirements and preparation See the General Instructions for all pipelines first. To run this pipeline the type of species and a folder with .fasta or paired .fastq files is required. Place all your samples in one folder, no subfolders, all file types should be the same format. Option is to choose between .fasta and .fastq, but those cannot be combined. Besides giving input it is also required to select a species. The --species parameter is required for PointFinder in order to find the chromosomal mutations. If you dont know the species you can fill this parameter with other. This way the pipeline will run ResFinder without running PointFinder. The output of the pipeline will be collected in the folder output within the working directory (the directory where the pipeline is active). If you want the output to be in a different location it is possible to do this with the --output parameter. Note: all samples in the directory should be the same species and the same file format. 5.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-amr pipeline can be found here. 5.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-amr Run the pipeline python3 juno-amr.py -s [species] -i [/mnt/scratch_dir/&lt;my_folder&gt;/fastq_or_fasta_file folder] Please read the section What to expect while running a Juno pipeline. See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You can add the --output parameter to the command in order to place your output somewhere else. 5.1.4 Output In the output you can find 3 folders: log: Log with output and error file from the cluster for each snakemake rule/step that is performed. results_per_sample: Output produced by ResFinder and PointFinder for each sample. CGEs explanation on the output can be found here. summary: Directory with 4 summary files created from each sample within the results_per_sample folder. summary_amr_genes.csv: Shows the samplename resistance gene, identity, alignment length/gene length and coverage per gene hit. summary_amr_phenotype.csv: Shows the type of match for each sample on the resistance for each antimicrobial. summary_amr_pointfinder_predicition.csv: - Shows the samplename and a 0(no hit) or 1(hit) prediciton for each mutation. summary_amr_pointfinder_results Shows the samplename, mutation, nucleotide change, amino acid change, resistance and PMID for each mutation hit Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o or --output python3 juno-amr.py -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -species salmonella -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ 5.1.5 Troubleshooting for this pipeline Please read the General Troubleshooting section first! 5.1.5.1 Other problems or failing rules The Juno-amr pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "],["juno-annotation.html", "6 Juno-annotation 6.1 Handbook", " 6 Juno-annotation body { text-align: justify} This pipeline takes assemblies (.fasta) as input and performs gene annotation. It should be able to annotate bacterial genomes and bacterial plasmids. The pipeline follows these steps: Filtering contigs with more than 200bp. This step is necessary for downstream tools to run properly and, in general, small contigs are often filtered to reduce the noise of possible contamination. Besides, these small contigs often do not contain much valuable information. Finding the start of the chromosome ( Circlator ). Although in most cases, the chromosome is already circularized after the assembly, in the cases in which that was not possible, this step might help. The start of the chromosome will be set at the beginning of the dnaA gene. For the plasmids or other smaller contigs, the start of a predicted gene near its center is used. Annotation using Prokka. Prokka is a fast program for prokaryotic genome annotation. We enriched the databases that Prokka relies on by using the RefSeq database. This step is very slow if run locally (~ 3 hours) but in the cluster it takes around 30 min per sample. 6.1 Handbook 6.1.1 Requirements and preparation See the General Instructions for all pipelines first. The pipeline needs to know the Genus and Species of each input fasta file. There are three ways to do this (see the section Run the pipeline for more details on how to do this and the recognized abbreviations): The pipeline can guess this information from the file name provided the appropriate abbreviations are used within the name. You can provide a metadata file (.csv) that should contain at least three columns: File_name, Genus and Species (mind the capital letters). The File name is case sensitive, so the names of the files (without the full path) should coincide EXACTLY with your input fasta files. The genus and species should be recognized as an official TaxID. You MUST write the genus and the species in the appropriate column, never together in one column. This option cannot be combined with the first one. This means that if you decide to guess the genus and species from the file name, the provided metadata file will be ignored. You can provide the genus and species directly when calling the pipeline. The genus and species should be recognized as an official TaxID. YOU CAN ONLY PROVIDE ONE GENUS AND ONE SPECIES and this will be used for all samples. You can combine this option with one of the two above. This means that if you have multiple samples but some of them are not enlisted in your metadata file, they will instead inherit the genus and species from the information provided directly when calling the pipeline. If all the files were enlisted in the metadata, this option will be ignored. 6.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-annotation pipeline can be found in this link. 6.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-annotation Run the pipeline If all your samples have the same genus and species, for instance, Klebsiella pneumoniae, you run it like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --genus Klebsiella --species pneumoniae Note that the genus and species should be ONE word. Do not put genus and species together! If your samples do not have all the same genus, you can make a metadata file. This file should be a .csv file with at least these columns and information: File_name Genus Species sample1_Kpn.fasta Klebsiella pneumoniae sample2Pae2.fasta Pseudomonas aeruginosa sample3Sau_1.fasta Staphylococcus aureus Note that the name of the columns should be EXACTLY the same than in this example, including the underscores and the capital letters. You would then call the pipeline like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --metadata path/to/metadata.csv Alternatively, if your input files have one of the following abbreviations somewhere in the data, the genus and species may be automatically guessed from the file name: Abbreviation Genus Species Cam Citrobacter amalonaticus Cbr Citrobacter braakii Cfr Citrobacter freundii Cse Citrobacter sedlakii Cwe Citrobacter werkmanii Ebu Enterobacter bugandensis Eca Enterobacter cancerogenus Ecl Enterobacter cloacae Eco Escherichia coli Exi Enterobacter cloacae Kae Klebsiella aerogenes Kox Klebsiella oxytoca Kpn Klebsiella pneumoniae Mmo Morganella morganii Pae Pseudomonas aeruginosa Pmi Proteus mirabilis Rpl Raoultella planticola Sar Staphylococcus argenteus Sau Staphylococcus aureus Sma Serratia marcescens Then you would call the pipeline like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --make-metadata Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 6.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (circlator, filtered_contigs and prokka). Please refer to the manuals of every tool to interpret the results. Your main result will be two genebank (.gbk) files per sample that contained your annotated genomes/plasmids. You can find them inside the subfolders out/prokka. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (from output) bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 6.1.5 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 6.1.5.1 Failure to find metadata file You could get an error associated to the metadata: The provided species file &lt;your_metadata_file.csv&gt; does not exist. Please provide an existing file &quot;If you used the option --make-metadata, please check that all the fasta files contain the .fasta extension and that the file names have the right abbreviations for genus/species This message means that either you provided the wrong path/name to your metadata file, that this has the wrong extension (not .csv) or that, if you used the option to --make-metadata, your input files do not have the right abbreviations. Please check the metadata file provided or the file names and try again. 6.1.5.2 Other problems or failing rules The Juno-annotation pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "],["juno-blast.html", "7 Juno_blast 7.1 Handbook", " 7 Juno_blast body { text-align: justify} The goal of this pipeline is to perform BLAST in the input file(s) contained in the input directory. The input file should be a (multi) fasta file. 7.1 Handbook 7.1.1 Requirements and preparation See the General Instructions for all pipelines first. This pipeline requires one .fasta file per sample. Note that the input files MUST have the extension .fasta. An output file will be created inside the output directory with the sample name as prefix. That file should have your results for BLAST. Note that if your fasta file has more than one sequence, the output of all of them will be included in the result file. 7.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno_blast pipeline can be found in this link. 7.1.3 Install conda environment YOU NEED TO REINSTALL THE MASTER ENVIRONMENT EVERY TIME YOU UPDATE THE PIPELINE (everytime you download the code) Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_blast If you already had a juno_blast environment before you need to delete the old one by using the command: conda env remove -n juno_blast If you had never created a juno_blast environment before, you can skip this step and go to step 4 instead. Create a new environment for running Juno_blast by using the command: conda env create -f envs/master_env.yaml This step will take some time (few minutes). Note: If this step would take more than 1 hour, please kill the process (using Ctrl + C or Ctrl + Z) and refer to the section General Troubleshooting. The first issue written there (Failure when installing master environment) often solves the problem. If, however, the problem persists, please contact me by email. 7.1.4 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_blast Activate juno_blast environment conda activate juno_blast Run the pipeline by providing an input directory: python juno_blast -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ Please read the section What to expect while running a Juno pipeline. See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 7.1.5 Output A folder called output/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) python juno_blast -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later time point and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 7.1.6 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 7.1.6.1 Other problems or failing rules The Juno_blast pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "],["Myco-lofreq.html", "8 Low frequency variants in Mycobacterium samples 8.1 Handbook", " 8 Low frequency variants in Mycobacterium samples body { text-align: justify} The main purpose of this pipeline is to call minority variants in Mycobacterium samples. I takes paired-end raw fastq files as input (one should contain R1 and the other R2 on the name). The pipeline performs the following steps: Quality Control of the raw reads using FastQC Trimming with Trimmomatic Quality Control of the trimmed reads using FastQC Alignment to reference genome using BWA Preparation to load in LoFreq Calling SNP and indels using LoFreq Annotating the resulting vcf file using vcf-annotator Although the pipeline has been written for Mycobacterium, it can easily be extended to other type of bacteria. Please contact me if you want to use the pipeline for other organism. 8.1 Handbook 8.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your files have the right format: they must have a fastq extension (.fastq, .fq, .fastq.gz or .fq.gz) and contain the characters R1 (for forward reads) or R2 (for reverse reads) somewhere on the name. As with the folder name, you should avoid rare characters on your file names. Just use letters, numbers, underscores or dashes. Make sure there are no spaces on the file names. 8.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Myco-lofreq pipeline can be found in this link. 8.1.3 Start the analysis. Basics Open the terminal. You can go to the Linux menu called Applications and open the program terminal or the terminator one. Both should work. Enter the folder of the pipeline cd /mnt/scratch_dir/&lt;my_folder&gt;/Myco_lofreq Run the pipeline bash run_myco_lofreq_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 8.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (fastqc, trimmomatic, multiqc, bwa_alignment, lofreq, etc). Please refer to the manuals of every tool to interpret the results. There are two important subfolders generated by this pipeline: A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (kmerfinder, mlst7 and serotype). Please refer to the manuals of every tool to interpret the results. Each one of these folders, there should be a sub-folder per sample and, for the case of mlst7 and serotype, also a csv file collecting the results of all the samples together: a serotype/salmonella_serotype_multireport.csv, serotype/ecoli_serotype_multireport.csv and mlst7/mlst7_multireport.csv. Even if your samples are not Salmonella or E. coli you will get the multireport file, althought it will be empty. Although the results of kmerfinder are provided, these have not been validated. They are used only to choose the right scheme for the MLST and the right serotyper. If you would use the results of kmerfinder as a species identification tool, you do it under your own risk and you should be able to interpret correctly the results. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash run_myco_lofreq_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 8.1.5 Troubleshooting Please read first the General Troubleshooting section! 8.1.5.1 Other problems or failing rules The Myco-lofreq pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "],["juno-deinterleave.html", "9 Juno-deinterleave 9.1 Handbook", " 9 Juno-deinterleave body { text-align: justify} # Parameters parameters &lt;- list(&quot;pipeline_name&quot; = &quot;Deinterleave&quot;) 9.1 Handbook 9.1.1 Requirements and preparation See the General Instructions for all pipelines first. 9.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Myco-lofreq pipeline can be found in this link. 9.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/deinterleave Run the deinterleave script bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz 9.1.4 Output Your two deinterleaved files will be located in the current directory (you can check your current directory by running the command pwd) and they will have the same name than the original file, but with the suffix \"_R1\" for the forward reads and \"_R2\" for the reverse reads. For example: Original file: sampleX.fastq Deinterleaved files: Forward: sampleX_R1.fastq Reverse: sampleX_R2.fastq Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (from output) bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz -o /mnt/scratch_dir/my_results 9.1.5 Extra options Choose name of deinterleaved files You can also use the names you want for the output files. For that you can do: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz --output_r1 my_deinterleaved_file_R1.fastq --output_r2 my_deinterleaved_file_R2.fastq Note that I did not use paths there. Your output directory should be set with the option -o Compress output You may want to have the deinterleaved files compressed. Note that the compression might take a bit long. For that you can do: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz --compress That will make sure that your two interleaved files are gzipped (and have the extension .fastq.gz). Combine options It is possible to combine more than one of this options. For example: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz -o /mnt/scratch_dir/my_results --compress "],["triumph-run.html", "10 Triumph: Run pipeline 10.1 Handbook", " 10 Triumph: Run pipeline body { text-align: justify} This pipeline processes raw fastq files from microbiome data using the Dada2 R-package. At the end, it also produces a report summary of the run. The pipeline has been tailor-made for the microbiome group at the RIVM, especifically for the Pienter project. It therefore expects the input that is normally generated there and might not be very flexible with sample names and metadata. I will try to explain what is expected from the input when necessary. 10.1 Handbook 10.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your files have the right format: they must have a fastq extension (.fastq, .fq, .fastq.gz or .fq.gz) and have specific names. The names should be only numbers (they can be preceded by 1 letter). The numbers should give information about the run and the sample. For instance, in the file name L002-026_R1.fastq.gz, it means that you are processing the project L, the run # 2 and the sample 026. Moreover, the suffix _R1 or _R2 designate forward or reverse reads. The MiSeq sequencer normally adds an extra suffix _0001 to the names of the fastq files L002-026_R1_0001.fastq.gz. These suffix is no problem for the pipeline to work. This pipeline needs a metadata (Excel file) should have a very specific format. For details about how to make the metadata file and what the codes mean, please consult with the microbiome team at the RIVM, especially Susana Fuentes. For the use of the pipeline, the main features that need to be added in this Excel file are: It should have the file extension .xlsx file (no .csv). It should have at least a sheet called amplicon_assay. The metadata file for the Triumph project includes many other sheets but this is the only one that is necessary. Leaving the sheet unnamed will not work even if the information inside the sheet is correctly formatted. The amplicon_assay sheet should have at least three columns with the names: assay_sample, sample_indentifier and subject_identifier. The assay_sample column should contain the root of the sample names that are used to name the fastq files (see previous point). For instance, the assay_sample code for the file L002-026_R1_0001.fastq.gz is L002-026. The sample_identifier column should contain the identity of the samples. Here specific codes are needed for the control samples. For instance, every non-control sample is named something like S12345678 where S denotes that it is a sample belonging to a patient and the numbers are a unique ID for that sample. In contrast, the control samples dont have unique IDs. They are always named the same way in every run. The accepted codes for the control samples can be found in the Table 1. The subject_identifier column contains the same code than sample_identifier and it is used as a back-up. When the codes in sample_identifier are not found, they are looked for instead in the subject_identifier column. TABLE 10.1: Table 1. Accepted subject_identifier codes for metadata used in the Triumph project. subject_identifier subject_description ZMCD Zymo mock DNA ZMCB Zymo mock Bacteria AMCD ATCC mock DNA UMD UMCU low density mock DNA MSS Mixed (fecal/NP/OP) sample control MSD Mixed (fecal/NP/OP) sample control DNA BD Blank from DNA extraction BP Blank from PCR LSIC Library spike-in control 10.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The triumph_microbiome_run pipeline can be found in this link. 10.1.3 Start the analysis Open the terminal. Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/triumph_dada2_run_pipeline-master/ Exact name of downloaded folder might be slightly different depending on the version that was downloaded. Run the pipeline bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ --metadata /path/to/metadata.xlsx Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 10.1.3.1 Run the pipeline changing default parameters Many of the parameters of the dada2 package can be changed/used directly when calling the pipeline. If you want to do that, please refer to the help of the pipeline by using the command: bash triumph_run_pipeline.sh -h It should print something like this: Usage: bash triumph_run_pipeline.sh -i &lt;INPUT_DIR&gt; &lt;parameters&gt; Input: -i, --input [DIR] This is the folder containing the input (demultiplexed) fastq files. This should be a folder for one single run (sub-folder name will be considered the run name! so give it a meaningul one). Default is the current folder. --metadata [xlsx file] (optional) This is an optional parameter in which you provide an xlsx file with the metadata. For the exact format of this file, you should refer to the microbiome group at the RIVM (Susana Fuentes). In summary, it should contain a sheet called &#39;amplicon_assay&#39;, with an empty row at the top (it does not need to be empty, but that row will be ignored) and then a table with at least two columns: assay_sample and sample_identifier. The assay_sample should have the names coinciding with the fastq files (so, like the sample sheet used for demultiplexing) and the sample_identifier should contain either the sample code or one of the codes for recognizing control samples (ask the microbiome group). Main Output (automatically generated): out/ Contains detailed intermediate files. out/log/ Contains all log files. out/run_reports/ Folder containint the actual reports of the run (html file) and the summary table of the run (used to generate that report) as a csv file. Parameters: -o, --output Output directory. Default is &#39;out/&#39; --pienter-up If this option is chosen, the default options for the nasopharynx Triumph project will be used, which means that truncLenF will be overwritten to 200 and the truncLenR will be overwritten to 150. -h, --help Print this help document. -sh, --snakemake-help Print the Snakemake this help document. --clean (-y) Removes output. (-y forces &#39;Yes&#39; on all prompts) -u, --unlock Removes the lock on the working directory. This happens when a run ends abruptly and prevents you from doing subsequent analyses. -n, --dry_run Shows the process but does not actually run the pipeline. Dada2 parameters that can be changed in this pipeline (if you decide to be able to directly modify another parameter when you call the pipeline, contact me alejandra.hernandez.segura@rivm.nl) for more details: --trunQ Minimumn quality for truncating a read (part of dada2 filtering). Default: 2 --truncLenF Truncating length for forward reads(part of dada2 filtering). Default: 220 --truncLenR Truncating length for reverse reads(part of dada2 filtering). Default: 100 --maxN Maximum number of N bases accepted per read (part of dada2 filtering). Default: 0. Note that the dada2 requires this parameter to be 0. --maxEE Maximum Expected Errors (part of dada2 filtering). Default: 2 --nomatchID If this file is present, the reads will not be expected to match. The default is to expect filtered reads to have matching IDs (part of dada2 filtering). Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --nbases Number of bases to use in order to learn the errors (part of dada2 error learning). Default: 1e9 --minlen Minimum length of the inferred SAVs. Any SAV shorter than minlen will be discarded (part of dada2 inferring SAVs). Default: 250 --maxlen Maximum length of the inferred SAVs. Any SAV longer than maxlen will be discarded (part of dada2 inferring SAVs). Default: 256 --trainset Path to trainset used to assign taxonomy to a phyloseq object. Dada2 suggests a database in its tutorial. In the RIVM, we have a copy stored at: &#39;/mnt/db/triumph_taxonomy_db/silva_nr_v138_train_set.fa.gz&#39; and this path is the default. --species_db Path to database used to assign species name to a phyloseq object. Dada2 suggests a database in its tutorial. In the RIVM, we have a copy stored at: &#39;/mnt/db/triumph_taxonomy_db/silva_species_assignment_v138.fa.gz&#39; and this path is the default. --no-randomize-errors, -nr If this flag is present, there will not be randomiza- tion while learning the errors (part of dada2 learn errors). The default is to randomize. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --just-concatenate If this flag is present, the pairs will be simply concatenated. (part of dada2 mergePairs). The default is set to FALSE. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE. --bimera_method Choose a method to remove bimeras (according to dada2 removeBimeraDenovo). --minboot The minimum bootstrap confidence for assigning a taxonomic level (part of dada2 assignTaxonomy). Default: 80 --no-tryrc If TRUE, the reverse-complement of each sequences will be used for classification if it is a better match to the reference sequences than the forward sequence. The default is TRUE. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --species-allowed This is the number of species that may be enlisted (part of dada2 addSpecies). Default is 3. 10.1.3.2 Running when metadata is not available You can also run the pipeline without the metadata: bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ However, the report produced of such a run is not very informative and the code is not actively maintained. Therefore, the report might fail. The pipeline is not intended to be run without metadata so it will do its job but not efficiently. 10.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. If you chose another site for the output, then you will find the same results there. This folder will contain all the results and logging files of your analysis. There will be one folder with all the filtered_fastq files, one with the plots and one with the RDS_files generated when using the dada2 package (for instance, seqtab, merged files, etc.). These RDS files can be loaded into R where you can explore them and further work with them. The last folder is the run_reports which will have the html report with statistics of the run and the Project_SummaryTable.csv with all the statistics in a table format. Please refer to the dada2 documentation to interpret your results. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 10.1.5 Troubleshooting Please read first the General Troubleshooting section! 10.1.5.1 Other problems or failing rules The triumph_microbiome_run pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "],["triumph-project.html", "11 Triumph: Project report 11.1 Handbook", " 11 Triumph: Project report body { text-align: justify} The code here generates a report that collects the results of multiple runs of the Triumph report. Note that in order to use it, all the individual runs should have been analyzed with the Triumph: Run pipeline 11.1 Handbook 11.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your folder has the right name and structure. Your input folder should have one subfolder per different run. Each of this subfolders contains the results of the Triumph: Run pipeline for that specific run. Please, do not rename any of the files produced by the Triumph: Run pipeline, otherwise they will not be found and cannot be included in the project report. Neither your folder nor your subfolders can have rare characters (only letters, numbers, dashes and underscores accepted). If your folder name contains different characters, it may not be recognized by the pipeline. IMPORTANT: Your folder name MUST NOT CONTAIN SPACES!!!! 11.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The triumph_microbiome_project pipeline can be found in this link. 11.1.3 Start the analysis. Basics Open the terminal. Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/triumph_project_report-master/ Exact name of downloaded folder might be slightly different depending on the version that was downloaded. Run the pipeline bash create_triumph_report.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/ The input directory (in this case /mnt/scratch_dir/my_folder) should be a folder containing one subfolder per run you want to include in the project report. These subfolders should contain the results of running the Triumph run pipeline using the --metadata option. Refer to the manual of that pipeline to see what that argument means. Note 1: If you are updating the project report because you added the analysis of an old run, you should first delete or rename the old report. This script might fail otherwise. Note 2: THE PROJECT REPORT PIPELINE ONLY WORKS WHEN ALL RUNS COLLECTED WERE EVALUATED WITH THE Triumph: Run pipeline USING A METADATA FILE!!! 11.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. If you prefer to store your output in a different folder, you can use the -o argument to give another path. bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ If you chose another site for the output, then you will find the same results there. In both cases, the output folder will contain the Triumph_Project_report.html file that is the actual report and a Triumph_Project_Report.log that contains messages, warnings and errors from R, where you can find more info if something goes wrong and the report could not be produced. 11.1.5 Troubleshooting Please read first the General Troubleshooting section! 11.1.5.1 Other problems or failing rules The triumph_microbiome_project pipeline is still in development which means that sometimes the process can fail. Make sure you ran the pipeline with the Triumph_dada2_run_pipeline and that you did not change the name of any file. Try to create the report again. If you are sure all the input files are correct and the report does not run try the two steps below before contacting for help: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be an delay in troubleshooting the problem. "]]
