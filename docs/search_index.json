[["index.html", "Bioinformatics Pipelines for Bacteriology (IDS, RIVM) 1 Introduction and General Instructions 1.1 General Instructions for all pipelines", " Bioinformatics Pipelines for Bacteriology (IDS, RIVM) Alejandra Hernandez-Segura 2021-06-11 1 Introduction and General Instructions In this book you will find the handbooks of all the available pipelines for the Bacteriology and Parasitology Department of the IDS (RIVM). The handbooks are updated regularly but if you would find a mistake or something that is not up-to-date, please communicate with: Alejandra Hernandez ( email ) Important note: This handbook is in continuous development, so please keep that in mind and contact Alejandra Hernandez if you wish to add/change something or if you need help with troubleshooting (please read the troubleshooting section for each pipeline first). 1.1 General Instructions for all pipelines 1.1.1 Requirements and preparation This handbook assumes that you are working at the bioinformatica environment at the RIVM. It is possible to run the pipeline in other settings and even on your laptop but you need extra steps that will not be enlisted here. Placing of the data: Your data should all be placed in one single folder (no subfolders) in the BioGrid (/data/BioGrid/&lt;my_folder&gt;/&lt;my_data&gt;/) or in the scratch_dir folder (/mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/). I strongly advice you to place it in the scratch_dir folder and to only copy it later to the BioGrid after the analysis. The run will be faster and therefore you will block the cluster less time for everyone else. Make sure that the folder that you use as input (it contains your input data) has the right name. This means: the folder that contains your data can have any name you want provided that you only use letters, numbers or underscores. If your folder name contains different characters, it may not be recognized by the pipeline. IMPORTANT: Your folder name and the PATH to it (so all the folders and subfolders that you have to enter to reach your folder) should NOT CONTAIN SPACES!!!! Make sure that your files have the right format: if they are fastq files, they should have the extension (.fastq, .fq, .fastq.gz or .fq.gz). If they are fasta files, they should have the extension .fasta. Any other requirements in the input files will be specified in the section of that specific pipeline. 1.1.2 Downloading pipelines All the bacteriology pipelines created by the IDS-bioinformatics group are stored in either GitHub or the internal GitLab of the RIVM. Only people who belong to the RIVM and that are inside one of our servers/environments can access to the later one with their normal RIVM login details. If you are going to download a pipeline, please do so in the same partition that your data is (preferentially scratch_dir). Each pipeline handbook has the instructions on where to find the code (either GitHub or GitLab). You can download every pipeline through the website or through the command line: GitHub website Go to the website for the pipeline (check the section of the specific pipeline). Press the green button Code on the page and then click on Download zip (see Figure 1.1 for explanation). A zip file (-master.zip) will have likely be downloaded on your Downloads folder. Please move this zip file to the BioGrid or the scratch_dir partitions, depending on where your data is. Extract the files of the zip file. In Linux this is normally done by pressing the left button of the mouse, then Open with Archive Manager and then press Extract on the two windows that will consecutively appear. You could then delete the zip file (see Figure 1.2 for explanation). FIGURE 1.1: Pipelines can be downloaded directly from their GitHub website. FIGURE 1.2: Unzipping a repository in Linux. GitLab website Go to the website for the pipeline (check the section of the specific pipeline). Press the small white button with a cloud and a downwards arrow. In the drop-down menu, choose Download zip (see Figure 1.3 for explanation). A zip file (-master.zip) will have likely be downloaded on your Downloads folder. Please move this zip file to the BioGrid or the scratch_dir partitions, depending on where your data is. Extract the files of the zip file. In Linux this is normally done by pressing the left button of the mouse, then Open with Archive Manager and then press Extract on the two windows that will consecutively appear. You could then delete the zip file (see Figure 1.2 above for explanation). FIGURE 1.3: Pipelines can be downloaded directly from their GitLab website. Command-line Any member of the RIVM has a (RIVM-specific) GitLab account. You can log in with the same credentials that you use for accessing your workspaces. In the case of pipelines hosted on GitHub, you may need a free GitHub account. Open the terminal (you could also open terminator) by going to the Applications menu of the linux environment. FIGURE 1.4: Pipelines can be downloaded directly from their GitLab website. Go to the location where you want to download the pipeline using the command cd. For instance: cd /mnt/scratch_dir/&lt;my_folder&gt;/ Note: mind the slash at the beginning of the path Download the pipeline using the git clone command git clone &lt;url_to_the_pipeline&gt;.git Note: notice that I added .git to the URL of the pipeline. You will be asked to give your credentials (username + password) and then the (already unzipped) pipeline should have been downloaded in your current folder. 1.1.3 What to expect while running a Juno pipeline Our pipelines normally run by themselves. Sometimes they ask for input from the user to agree on installing software or a database but other than that, they do not require much input from the user.Try to read at least the last lines appearing on the screen and if you get asked to give a permission, please do. The first time you run a new pipeline, the preparation might take longer than expected. This is due to the installation of the basic software needed for it to work. Be patient! The installation may take some time. Once the installation is over, the pipeline should start running. Please check regularly that the pipeline is actually running and working. Every time you will get yellow or green messages that tell you what is happening and that indicate that the pipeline is working as expected. Multiple steps are performed simultaneously to optimize the time and resources, especially when using the cluster. If a step fails, you will likely see some red text appearing on the screen, but the pipeline might keep going to finish all the steps that are possible. The messages on the screen regularly tell you how far the pipeline is (it gives a % of the steps that have been finished). Once your pipeline is finished you should see some final messages informing you that 100% of the steps were finished and that your output was created. See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 1.1.4 General output and log files for every pipeline Every pipeline (with few exceptions of the very short ones like the deinterleave one) generates some extra output that is not essential for your analysis but that it is required for debugging/troubleshooting. Especially, if you are contacting someone at the IDS-bioinformatics team, we expect you to have these files at hand or to also send the location where they can be found (make sure that we have right to access that location or move them to a shared folder where they can be accessed). The log/ subfolder is located inside your output directory. It contains one file per every step performed by the pipeline for each sample. There you can find error messages or some information of what happened during each step. The messages are not always easy to interpret, but they often have clues on why a job/analysis failed. Sometimes the log files for each tool (and sample) are empty because either, there were no problems or messages generated on the run or because the problem lies before the job/analysis was even started. You can look at them of course, but it is ok if you do not understand the messages. However, for us as bioinformaticians it is essential to have access to these files when something goes wrong inside one of the scripts/tools used. The log/drmaa/ subfolder is also inside the output directory and inside the log subdirectory. Here you can find other type of logging files of any job performed by the pipeline. This is even more technical logging but it can be veru useful to debug system errors, problems with memory, etc. The audit_trail or results subfolders are also located inside the output directory. They contain contain 4 very important files for trace-ability of your samples. The log_conda.txt file contains information about the software that was necessary and that was contained in your environment. This means basically the software that would be needed to reproduce the same circumstances in which the pipeline was run and how it can be reproduced. The log_config.txt file is even more informative. It enlists all the parameters used to run the pipeline. In case months later you forgot how you got the results you did or you just want to know some details about the analyses, they are all stored there. The log_git.txt has information about the repository or the code that was downloaded. It tells you exactly how it was downloaded so you can reproduce it at a later time point. Finally, the snakemake_report.html has a nice overview of the different steps that were performed with your samples, when were they performed, which output was produced and which software was used, as well as some statistics on how the run went. Even though you would not look at this output regularly, it is useful for us as bioinformaticians to be able to help you if a porblem would arise. However, they are also useful for you! Especially the audit_trail has the information you would need to put in a publication (software versions, parameters, etc.). Please do not delete them and know they are important. "],["general-troubleshooting.html", "2 General Troubleshooting 2.1 Failure when installing master environment 2.2 Failure to make a sample sheet or to find input directory 2.3 Other problems or failing rules", " 2 General Troubleshooting body { text-align: justify} 2.1 Failure when installing master environment Sometimes the installation does not work because of the settings of Linux. The installation always takes some minutes in which a message saying Solving environment... might appear on the screen. However, if this step takes too long (for instance 30 min or more) please abort the step by pressing Ctrl + C and/or Ctrl + Z (sometimes you need to press repeateadly). In such cases, please change the necessary settings with this command: conda config --set channel_priority false Then try to run the pipeline and install the master environment again. 2.2 Failure to make a sample sheet or to find input directory There are few reasons why this could happen: Your path (all the folders that lead to your input directory) contains spaces or unrecognized characters. You mis-spelled something in your path or gave the wrong path. Especially while working in Linux, it is common to forget a slash (/) at the beginning of a full path. For instance: /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/. Your files might have the incorrect format and/or extension. Normally for all the files, standard names are accepted (for instance, .fastq.gz or .fastq for raw reads and .fasta for fasta files). Please make sure that your files have the correct and classical extensions. Your input directory does nor contain the expected files, but they are instead in sub-folders. Most pipelines expect all input files to be together in one folder instead of scattered in sub-folders. The only exception is when you use output from a previous pipeline (for instance, Juno-assembly) into another pipeline. Please collect all your samples together in one folder. 2.3 Other problems or failing rules The Juno pipelines are still in development which means that sometimes things fail. Before contacting me for help, please try these steps: Re-run the pipeline again and see if it goes further. If it does, please keep re-running the pipeline until your analysis is finished or it just doesnt go further. Even if you are able to finish your analysis, just send me an email afterwards (see step 3) so I can check what happened. Download the pipeline again and start from the beginning of this handbook. Sometimes the issue has been resolved in newer versions of the pipeline. Collect your logging files and contact me. Please inform me about bugs/errors via e-mail sending also your log files and the path where I can find your input directory and the pipeline. No screenshots are necessary. Note that if you do not send this information, I will not be able to help you and your and my work will be delayed. "],["juno-assembly.html", "3 Juno-assembly pipeline 3.1 Handbook", " 3 Juno-assembly pipeline body { text-align: justify} The goal of this pipeline is to generate assemblies from raw fastq files. The input of the pipeline is raw Illumina paired-end data in the form of two fastq files (with extension .fastq, .fastq.gz, .fq or .fq.gz), containing the forward and the reversed reads (R1 and R2 must be part of the file name, respectively). On the basis of the generated genome assemblies, low quality and contaminated samples can be excluded for downstream analysis. Note: The pipeline has been tested only in gastroenteric bacteria ( Salmonella, Shigella, Listeria and STEC) but it could theoretically be used in other genera/species. The pipeline uses the following tools: FastQC (Andrews, 2010) is used to assess the quality of the raw Illumina reads Trimmomatic (Bolger, Lohse, &amp; Usadel, 2014) is used to remove poor quality data and adapter sequences. The sliding window option of Trimmomatic starts scanning at the 5 end and clips the read once the average quality within the window falls below a threshold. The sliding window is the number of nucleotides over which Trimmomatic calculates an average phred quality score, a measure of the quality of the identification of the nucleobases generated by automated DNA sequencing. The Trimmomatic minlen config parameter is set to 50, this parameter is used to drop the read if the read is below a specific length. FastQC is used once more to assess the quality of the trimmed reads Picard determines the library fragment lengths The reads are assembled into scaffolds by SPAdes (Bankevich et al., 2012) by means of de novo assembly of the genome. SPAdes uses k-mers for building an initial de Bruijn graph and on following stages it performs graph-theoretical operations to assemble the genome. Kmer sizes of 21, 33, 55, 77 and 99 were used. For de novo assembly, SPAdes isolate mode is used. QUAST (Gurevich, Saveliev, Vyahhi, &amp; Tesler, 2013) is used to assess the quality of the filtered scaffolds. To assess the quality of the microbial genomes, CheckM (Parks, Imelfort, Skennerton, Hugenholtz, &amp; Tyson, 2015) is used. CheckM calculates scores for completeness, contamination and strain heterogeneity. Bbtools (Bushnell, 2014) is used to generate scaffold alignment metrics. MultiQC (Ewels, Magnusson, Lundin, &amp; Käller, 2016) is used to summarize analysis results and quality assessments in a single report for dynamic visualization. 3.1 Handbook 3.1.1 Requirements and preparation See the General Instructions for all pipelines first. You have different possibilities to run the complete pipeline. Your input directory should contain samples with just ONE genus type. If this is the case, you should tell the pipeline which genus you have (e.g. using the flag genus Salmonella). Read further for more details. You have to make sure that your sample was listed in the Excel file: /data/BioGrid/NGSlab/BAC_in_house_NGS/In-house_NGS_selectie_2021.xlsx (look at the first sheet) and that a genus was provided in the appropriate column. If this is not the case DO NOT MODIFY THAT EXCEL FILE! Instead follow the steps mentioned in the Troubleshooting section, particularly in the part Error saying that my samples cannot be found on the Excel file with the genus list You could skip the step that uses the CheckM tool. That step is the only one that actually requires the information about the genus. You would be missing the statistics about how complete the assembly is with respect to the reference genome. 3.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-assembly pipeline can be found in this link. 3.1.3 Start the analysis. Basics Open a terminal. (Applications &gt; terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_pipeline Run the pipeline If all your samples have the same genus, for instance, Salmonella, you run it like this: bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --genus Salmonella Note that the genus should be ONE word. Do not put any species names! If your samples do not have all the same genus but they are present in the file /data/BioGrid/NGSlab/BAC_in_house_NGS/In-house_NGS_selectie_2021.xlsx, then do: bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ If you do not manage to use any of these steps or you prefer to skip the step with CheckM (this step calculates genome completeness and gives a proxy of contamination) then do: bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --no-genus Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 3.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per result (qc_raw_fastq, clean_fastq, qc_clean_fastq, de_novo_assembly, de_novo_assembly_filtered, qc_de_novo_assembly). Please refer to the manuals of every tool to interpret the results. . Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 3.1.5 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 3.1.5.1 Error saying that the genus supplied was not recognized by CheckM If you get this message: ERROR: The genus supplied with the sample(s): Sample_name1 Sample_name2 were not recognized by CheckM Please supply the sample row in the Excel file /data/BioGrid/NGSlab/BAC_in_house_NGS/In-house_NGS_selectie_2021.xlsx with a correct genus. If you are unsure what genera are accepted by the current version of the pipeline, please run the pipeline using the --help-genera command to see available genera. it means that although you provided the genus information, it is probably not supported by the tool CheckM. First check that you have no spelling mistakes in the genus name of your sample(s) or that you used the correct case (first letter capital and the rest on small letters). If you are sure that your spelling is correct, it may be that CheckM does not have the genus you supplied on its database. As the message says, if you are not sure on how to spell the genus name or which genus is accepted, please check the list of accepted genera by typing. bash juno -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --help-genera 3.1.5.2 Other problems or failing rules The Juno-assembly pipeline is still in development which means that sometimes things fail. Before contacting me for help, please try these steps: Re-run the pipeline again and see if it goes further. If it does, please keep re-running the pipeline until your analyis is finished or it just doesnt go further. Even if you are able to finish your analysis, just send me an email afterwards (see step 3) so I can check what happened. Download the pipeline again and start from the beginning of this handbook. Sometimes the issue has been resolved in newer versions of the pipeline. Collect your logging files and contact me. Please inform me about bugs/errors via e-mail sending also your log files and the path where I can find your input directory and the pipeline. No screenshots are necessary. Note that if you do not send this information, I will not be able to help you and your and my work will be delayed. "],["juno-typing.html", "4 Juno-typing 4.1 Handbook", " 4 Juno-typing body { text-align: justify} The goal of this pipeline is to perform bacterial typing (7-locus MLST and serotyping). It takes 2 types of files per sample as input: Two .fastq files (paired-end sequencing) derived from short-read sequencing. They should be already filtered and trimmed (for instance, with the Juno-pipeline). An assembly from the same sample in the form of a single .fasta file. Importantly, the Juno-typing pipeline works directly on output generated from the Juno-assembly pipeline. The Juno-typing pipeline will then make the following analyses: 7-locus MLST (using the MLST tool from the CGE group). Serotyping Salmonella samples (using the SeqSero2 tool). Serotyping E. coli samples (using the SerotypeFinder tool). Disclaimer!! Importantly, the genus and species are automatically detected from the processed reads (using kmerFinder) and the schema to use for the 7-locus MLST is chosen accordingly. The results of kmerfinder are also included in the output. However, this pipeline is NOT meant for bacteria identification and it has not been tested for it. Even if similar species would be misidentified, they would probably share the same MLST7 scheme. Following that thought, the correct choice of scheme has been tested, but not the correct species identification. 4.1 Handbook 4.1.1 Requirements and preparation See the General Instructions for all pipelines first. This pipeline needs two fastq file (R1 and R2) and an assembly (.fasta) files per sample. The fastq files should have been trimmed and filtered to remove low quality reads/bases. You could use the Juno-assembly pipeline for that. Moreover, that pipeline also provides the de novo assembly for your samples and the output folder of the Juno-assembly pipeline can be used directly into the Juno-typing pipeline. If you, however, prefer to use any other tool for doing your assembly and trimming/filtering, make sure that the fastq files and fasta files have the same name (for instance, sample1_R1_001.fastq.gz, sample1_R1_001.fastq.gz and sample1.fasta). If that is not the case, the files may not be recoginzed as belonging to the same sample. Also, ALL THREE FILES SHOULD BE IN THE SAME FOLDER! If you have multiple samples, they should all be in the same input folder, NOT IN SUBFOLDERS. The only exception is if you use the Juno-assembly pipeline to pre-process your data. In that case, the pipeline will recognize the subfolders where the fastq files and the fasta files should be. 4.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-typing pipeline can be found in this link. 4.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-typing Run the pipeline bash juno-typing -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 4.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (kmerfinder, mlst7 and serotype). Please refer to the manuals of every tool to interpret the results. Each one of these folders, there should be a sub-folder per sample and, for the case of mlst7 and serotype, also a csv file collecting the results of all the samples together: a serotype/salmonella_serotype_multireport.csv, serotype/ecoli_serotype_multireport.csv and mlst7/mlst7_multireport.csv. Even if your samples are not Salmonella or E. coli you will get the multireport file, althought it will be empty. Although the results of kmerfinder are provided, these have not been validated. They are used only to choose the right scheme for the MLST and the right serotyper. If you would use the results of kmerfinder as a species identification tool, you do it under your own risk and you should be able to interpret correctly the results. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash juno-typing -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 4.1.5 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 4.1.5.1 Other problems or failing rules The Juno-typing pipeline is still in development which means that sometimes things fail. Before contacting me for help, please try these steps: Re-run the pipeline again and see if it goes further. If it does, please keep re-running the pipeline until your analyis is finished or it just doesnt go further. Even if you are able to finish your analysis, just send me an email afterwards (see step 3) so I can check what happened. Download the pipeline again and start from the beginning of this handbook. Sometimes the issue has been resolved in newer versions of the pipeline. Collect your logging files and contact me. Please inform me about bugs/errors via e-mail sending also your log files and the path where I can find your input directory and the pipeline. No screenshots are necessary. Note that if you do not send this information, I will not be able to help you and your and my work will be delayed. "],["juno-annotation.html", "5 Juno-annotation 5.1 Handbook", " 5 Juno-annotation body { text-align: justify} This pipeline takes assemblies (.fasta) as input and performs gene annotation. It should be able to annotate bacterial genomes and bacterial plasmids. The pipeline follows these steps: Filtering contigs with more than 200bp. This step is necessary for downstream tools to run properly and, in general, small contigs are often filtered to reduce the noise of possible contamination. Besides, these small contigs often do not contain much valuable information. Finding the start of the chromosome ( Circlator ). Although in most cases, the chromosome is already circularized after the assembly, in the cases in which that was not possible, this step might help. The start of the chromosome will be set at the beginning of the dnaA gene. For the plasmids or other smaller contigs, the start of a predicted gene near its center is used. Annotation using Prokka. Prokka is a fast program for prokaryotic genome annotation. We enriched the databases that Prokka relies on by using the RefSeq database. This step is very slow if run locally (~ 3 hours) but in the cluster it takes around 30 min per sample. 5.1 Handbook 5.1.1 Requirements and preparation See the General Instructions for all pipelines first. The pipeline needs to know the Genus and Species of each input fasta file. There are three ways to do this (see the section Run the pipeline for more details on how to do this and the recognized abbreviations): The pipeline can guess this information from the file name provided the appropriate abbreviations are used within the name. You can provide a metadata file (.csv) that should contain at least three columns: File_name, Genus and Species (mind the capital letters). The File name is case sensitive, so the names of the files (without the full path) should coincide EXACTLY with your input fasta files. The genus and species should be recognized as an official TaxID. You MUST write the genus and the species in the appropriate column, never together in one column. This option cannot be combined with the first one. This means that if you decide to guess the genus and species from the file name, the provided metadata file will be ignored. You can provide the genus and species directly when calling the pipeline. The genus and species should be recognized as an official TaxID. YOU CAN ONLY PROVIDE ONE GENUS AND ONE SPECIES and this will be used for all samples. You can combine this option with one of the two above. This means that if you have multiple samples but some of them are not enlisted in your metadata file, they will instead inherit the genus and species from the information provided directly when calling the pipeline. If all the files were enlisted in the metadata, this option will be ignored. 5.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-annotation pipeline can be found in this link. 5.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-annotation Run the pipeline If all your samples have the same genus and species, for instance, Klebsiella pneumoniae, you run it like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --genus Klebsiella --species pneumoniae Note that the genus and species should be ONE word. Do not put genus and species together! If your samples do not have all the same genus, you can make a metadata file. This file should be a .csv file with at least these columns and information: File_name Genus Species sample1_Kpn.fasta Klebsiella pneumoniae sample2Pae2.fasta Pseudomonas aeruginosa sample3Sau_1.fasta Staphylococcus aureus Note that the name of the columns should be EXACTLY the same than in this example, including the underscores and the capital letters. You would then call the pipeline like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --metadata path/to/metadata.csv Alternatively, if your input files have one of the following abbreviations somewhere in the data, the genus and species may be automatically guessed from the file name: Abbreviation Genus Species Cam Citrobacter amalonaticus Cbr Citrobacter braakii Cfr Citrobacter freundii Cse Citrobacter sedlakii Cwe Citrobacter werkmanii Ebu Enterobacter bugandensis Eca Enterobacter cancerogenus Ecl Enterobacter cloacae Eco Escherichia coli Exi Enterobacter cloacae Kae Klebsiella aerogenes Kox Klebsiella oxytoca Kpn Klebsiella pneumoniae Mmo Morganella morganii Pae Pseudomonas aeruginosa Pmi Proteus mirabilis Rpl Raoultella planticola Sar Staphylococcus argenteus Sau Staphylococcus aureus Sma Serratia marcescens Then you would call the pipeline like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --make-metadata Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 5.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (circlator, filtered_contigs and prokka). Please refer to the manuals of every tool to interpret the results. Your main result will be two genebank (.gbk) files per sample that contained your annotated genomes/plasmids. You can find them inside the subfolders out/prokka. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (from output) bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 5.1.5 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 5.1.5.1 Failure to find metadata file You could get an error associated to the metadata: The provided species file &lt;your_metadata_file.csv&gt; does not exist. Please provide an existing file &quot;If you used the option --make-metadata, please check that all the fasta files contain the .fasta extension and that the file names have the right abbreviations for genus/species This message means that either you provided the wrong path/name to your metadata file, that this has the wrong extension (not .csv) or that, if you used the option to --make-metadata, your input files do not have the right abbreviations. Please check the metadata file provided or the file names and try again. 5.1.5.2 Other problems or failing rules The Juno-annotation pipeline is still in development which means that sometimes things fail. Before contacting me for help, please try these steps: Re-run the pipeline again and see if it goes further. If it does, please keep re-running the pipeline until your analyis is finished or it just doesnt go further. Even if you are able to finish your analysis, just send me an email afterwards (see step 3) so I can check what happened. Download the pipeline again and start from the beginning of this handbook. Sometimes the issue has been resolved in newer versions of the pipeline. Collect your logging files and contact me. Please inform me about bugs/errors via e-mail sending also your log files and the path where I can find your input directory and the pipeline. No screenshots are necessary. Note that if you do not send this information, I will not be able to help you and your and my work will be delayed. "],["low-frequency-variants-in-mycobacterium-samples.html", "6 Low frequency variants in Mycobacterium samples 6.1 Handbook", " 6 Low frequency variants in Mycobacterium samples body { text-align: justify} The main purpose of this pipeline is to call minority variants in Mycobacterium samples. I takes paired-end raw fastq files as input (one should contain R1 and the other R2 on the name). The pipeline performs the following steps: Quality Control of the raw reads using FastQC Trimming with Trimmomatic Quality Control of the trimmed reads using FastQC Alignment to reference genome using BWA Preparation to load in LoFreq Calling SNP and indels using LoFreq Annotating the resulting vcf file using vcf-annotator Although the pipeline has been written for Mycobacterium, it can easily be extended to other type of bacteria. Please contact me if you want to use the pipeline for other organism. 6.1 Handbook 6.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your files have the right format: they must have a fastq extension (.fastq, .fq, .fastq.gz or .fq.gz) and contain the characters R1 (for forward reads) or R2 (for reverse reads) somewhere on the name. As with the folder name, you should avoid rare characters on your file names. Just use letters, numbers, underscores or dashes. Make sure there are no spaces on the file names. 6.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-annotation pipeline can be found in this link. 6.1.3 Start the analysis. Basics Open the terminal. You can go to the Linux menu called Applications and open the program terminal or the terminator one. Both should work. Enter the folder of the pipeline cd /mnt/scratch_dir/&lt;my_folder&gt;/Myco_lofreq Run the pipeline bash run_myco_lofreq_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 6.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (fastqc, trimmomatic, multiqc, bwa_alignment, lofreq, etc). Please refer to the manuals of every tool to interpret the results. There are two important subfolders generated by this pipeline: A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (kmerfinder, mlst7 and serotype). Please refer to the manuals of every tool to interpret the results. Each one of these folders, there should be a sub-folder per sample and, for the case of mlst7 and serotype, also a csv file collecting the results of all the samples together: a serotype/salmonella_serotype_multireport.csv, serotype/ecoli_serotype_multireport.csv and mlst7/mlst7_multireport.csv. Even if your samples are not Salmonella or E. coli you will get the multireport file, althought it will be empty. Although the results of kmerfinder are provided, these have not been validated. They are used only to choose the right scheme for the MLST and the right serotyper. If you would use the results of kmerfinder as a species identification tool, you do it under your own risk and you should be able to interpret correctly the results. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash run_myco_lofreq_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 6.1.5 Troubleshooting Please read first the General Troubleshooting section! 6.1.5.1 Other problems or failing rules The Juno-annotation pipeline is still in development which means that sometimes things fail. Before contacting me for help, please try these steps: Re-run the pipeline again and see if it goes further. If it does, please keep re-running the pipeline until your analyis is finished or it just doesnt go further. Even if you are able to finish your analysis, just send me an email afterwards (see step 3) so I can check what happened. Download the pipeline again and start from the beginning of this handbook. Sometimes the issue has been resolved in newer versions of the pipeline. Collect your logging files and contact me. Please inform me about bugs/errors via e-mail sending also your log files and the path where I can find your input directory and the pipeline. No screenshots are necessary. Note that if you do not send this information, I will not be able to help you and your and my work will be delayed. "],["juno-deinterleave.html", "7 Juno-deinterleave 7.1 Handbook", " 7 Juno-deinterleave body { text-align: justify} # Parameters parameters &lt;- list(&quot;pipeline_name&quot; = &quot;Deinterleave&quot;) 7.1 Handbook 7.1.1 Requirements and preparation See the General Instructions for all pipelines first. 7.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-annotation pipeline can be found in this link. 7.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/deinterleave Run the deinterleave script bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz 7.1.4 Output Your two deinterleaved files will be located in the current directory (you can check your current directory by running the command pwd) and they will have the same name than the original file, but with the suffix \"_R1\" for the forward reads and \"_R2\" for the reverse reads. For example: Original file: sampleX.fastq Deinterleaved files: Forward: sampleX_R1.fastq Reverse: sampleX_R2.fastq Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (from output) bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz -o /mnt/scratch_dir/my_results 7.1.5 Extra options Choose name of deinterleaved files You can also use the names you want for the output files. For that you can do: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz --output_r1 my_deinterleaved_file_R1.fastq --output_r2 my_deinterleaved_file_R2.fastq Note that I did not use paths there. Your output directory should be set with the option -o Compress output You may want to have the deinterleaved files compressed. Note that the compression might take a bit long. For that you can do: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz --compress That will make sure that your two interleaved files are gzipped (and have the extension .fastq.gz). Combine options It is possible to combine more than one of this options. For example: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz -o /mnt/scratch_dir/my_results --compress "],["triumph-run.html", "8 Triumph: Run pipeline 8.1 Handbook", " 8 Triumph: Run pipeline body { text-align: justify} This pipeline processes raw fastq files from microbiome data using the Dada2 R-package. At the end, it also produces a report summary of the run. The pipeline has been tailor-made for the microbiome group at the RIVM, especifically for the Pienter project. It therefore expects the input that is normally generated there and might not be very flexible with sample names and metadata. I will try to explain what is expected from the input when necessary. 8.1 Handbook 8.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your files have the right format: they must have a fastq extension (.fastq, .fq, .fastq.gz or .fq.gz) and have specific names. The names should be only numbers (they can be preceded by 1 letter). The numbers should give information about the run and the sample. For instance, in the file name L002-026_R1.fastq.gz, it means that you are processing the project L, the run # 2 and the sample 026. Moreover, the suffix _R1 or _R2 designate forward or reverse reads. The MiSeq sequencer normally adds an extra suffix _0001 to the names of the fastq files L002-026_R1_0001.fastq.gz. These suffix is no problem for the pipeline to work. This pipeline needs a metadata (Excel file) should have a very specific format. For details about how to make the metadata file and what the codes mean, please consult with the microbiome team at the RIVM, especially Susana Fuentes. For the use of the pipeline, the main features that need to be added in this Excel file are: It should have the file extension .xlsx file (no .csv). It should have at least a sheet called amplicon_assay. The metadata file for the Triumph project includes many other sheets but this is the only one that is necessary. Leaving the sheet unnamed will not work even if the information inside the sheet is correctly formatted. The amplicon_assay sheet should have at least three columns with the names: assay_sample, sample_indentifier and subject_identifier. The assay_sample column should contain the root of the sample names that are used to name the fastq files (see previous point). For instance, the assay_sample code for the file L002-026_R1_0001.fastq.gz is L002-026. The sample_identifier column should contain the identity of the samples. Here specific codes are needed for the control samples. For instance, every non-control sample is named something like S12345678 where S denotes that it is a sample belonging to a patient and the numbers are a unique ID for that sample. In contrast, the control samples dont have unique IDs. They are always named the same way in every run. The accepted codes for the control samples can be found in the Table 1. The subject_identifier column contains the same code than sample_identifier and it is used as a back-up. When the codes in sample_identifier are not found, they are looked for instead in the subject_identifier column. TABLE 8.1: Table 1. Accepted subject_identifier codes for metadata used in the Triumph project. subject_identifier subject_description ZMCD Zymo mock DNA ZMCB Zymo mock Bacteria AMCD ATCC mock DNA UMD UMCU low density mock DNA MSS Mixed (fecal/NP/OP) sample control MSD Mixed (fecal/NP/OP) sample control DNA BD Blank from DNA extraction BP Blank from PCR LSIC Library spike-in control 8.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The triumph_microbiome_run pipeline can be found in this link. 8.1.3 Start the analysis Open the terminal. Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/triumph_dada2_run_pipeline-master/ Exact name of downloaded folder might be slightly different depending on the version that was downloaded. Run the pipeline bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ --metadata /path/to/metadata.xlsx Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 8.1.3.1 Run the pipeline changing default parameters Many of the parameters of the dada2 package can be changed/used directly when calling the pipeline. If you want to do that, please refer to the help of the pipeline by using the command: bash triumph_run_pipeline.sh -h It should print something like this: Usage: bash triumph_run_pipeline.sh -i &lt;INPUT_DIR&gt; &lt;parameters&gt; Input: -i, --input [DIR] This is the folder containing the input (demultiplexed) fastq files. This should be a folder for one single run (sub-folder name will be considered the run name! so give it a meaningul one). Default is the current folder. --metadata [xlsx file] (optional) This is an optional parameter in which you provide an xlsx file with the metadata. For the exact format of this file, you should refer to the microbiome group at the RIVM (Susana Fuentes). In summary, it should contain a sheet called &#39;amplicon_assay&#39;, with an empty row at the top (it does not need to be empty, but that row will be ignored) and then a table with at least two columns: assay_sample and sample_identifier. The assay_sample should have the names coinciding with the fastq files (so, like the sample sheet used for demultiplexing) and the sample_identifier should contain either the sample code or one of the codes for recognizing control samples (ask the microbiome group). Main Output (automatically generated): out/ Contains detailed intermediate files. out/log/ Contains all log files. out/run_reports/ Folder containint the actual reports of the run (html file) and the summary table of the run (used to generate that report) as a csv file. Parameters: -o, --output Output directory. Default is &#39;out/&#39; --pienter-up If this option is chosen, the default options for the nasopharynx Triumph project will be used, which means that truncLenF will be overwritten to 200 and the truncLenR will be overwritten to 150. -h, --help Print this help document. -sh, --snakemake-help Print the Snakemake this help document. --clean (-y) Removes output. (-y forces &#39;Yes&#39; on all prompts) -u, --unlock Removes the lock on the working directory. This happens when a run ends abruptly and prevents you from doing subsequent analyses. -n, --dry_run Shows the process but does not actually run the pipeline. Dada2 parameters that can be changed in this pipeline (if you decide to be able to directly modify another parameter when you call the pipeline, contact me alejandra.hernandez.segura@rivm.nl) for more details: --trunQ Minimumn quality for truncating a read (part of dada2 filtering). Default: 2 --truncLenF Truncating length for forward reads(part of dada2 filtering). Default: 220 --truncLenR Truncating length for reverse reads(part of dada2 filtering). Default: 100 --maxN Maximum number of N bases accepted per read (part of dada2 filtering). Default: 0. Note that the dada2 requires this parameter to be 0. --maxEE Maximum Expected Errors (part of dada2 filtering). Default: 2 --nomatchID If this file is present, the reads will not be expected to match. The default is to expect filtered reads to have matching IDs (part of dada2 filtering). Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --nbases Number of bases to use in order to learn the errors (part of dada2 error learning). Default: 1e9 --minlen Minimum length of the inferred SAVs. Any SAV shorter than minlen will be discarded (part of dada2 inferring SAVs). Default: 250 --maxlen Maximum length of the inferred SAVs. Any SAV longer than maxlen will be discarded (part of dada2 inferring SAVs). Default: 256 --trainset Path to trainset used to assign taxonomy to a phyloseq object. Dada2 suggests a database in its tutorial. In the RIVM, we have a copy stored at: &#39;/mnt/db/triumph_taxonomy_db/silva_nr_v138_train_set.fa.gz&#39; and this path is the default. --species_db Path to database used to assign species name to a phyloseq object. Dada2 suggests a database in its tutorial. In the RIVM, we have a copy stored at: &#39;/mnt/db/triumph_taxonomy_db/silva_species_assignment_v138.fa.gz&#39; and this path is the default. --no-randomize-errors, -nr If this flag is present, there will not be randomiza- tion while learning the errors (part of dada2 learn errors). The default is to randomize. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --just-concatenate If this flag is present, the pairs will be simply concatenated. (part of dada2 mergePairs). The default is set to FALSE. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE. --bimera_method Choose a method to remove bimeras (according to dada2 removeBimeraDenovo). --minboot The minimum bootstrap confidence for assigning a taxonomic level (part of dada2 assignTaxonomy). Default: 80 --no-tryrc If TRUE, the reverse-complement of each sequences will be used for classification if it is a better match to the reference sequences than the forward sequence. The default is TRUE. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --species-allowed This is the number of species that may be enlisted (part of dada2 addSpecies). Default is 3. 8.1.3.2 Running when metadata is not available You can also run the pipeline without the metadata: bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ However, the report produced of such a run is not very informative and the code is not actively maintained. Therefore, the report might fail. The pipeline is not intended to be run without metadata so it will do its job but not efficiently. 8.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. If you chose another site for the output, then you will find the same results there. This folder will contain all the results and logging files of your analysis. There will be one folder with all the filtered_fastq files, one with the plots and one with the RDS_files generated when using the dada2 package (for instance, seqtab, merged files, etc.). These RDS files can be loaded into R where you can explore them and further work with them. The last folder is the run_reports which will have the html report with statistics of the run and the Project_SummaryTable.csv with all the statistics in a table format. Please refer to the dada2 documentation to interpret your results. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 8.1.5 Troubleshooting Please read first the General Troubleshooting section! 8.1.5.1 Other problems or failing rules The triumph_microbiome_run pipeline is still in development which means that sometimes things fail. Before contacting me for help, please try these steps: Re-run the pipeline again and see if it goes further. If it does, please keep re-running the pipeline until your analyis is finished or it just doesnt go further. Even if you are able to finish your analysis, just send me an email afterwards (see step 3) so I can check what happened. Download the pipeline again and start from the beginning of this handbook. Sometimes the issue has been resolved in newer versions of the pipeline. Collect your logging files and contact me. Please inform me about bugs/errors via e-mail sending also your log files and the path where I can find your input directory and the pipeline. No screenshots are necessary. Note that if you do not send this information, I will not be able to help you and your and my work will be delayed. "],["triumph-project-report.html", "9 Triumph: Project report 9.1 Handbook", " 9 Triumph: Project report body { text-align: justify} The code here generates a report that collects the results of multiple runs of the Triumph report. Note that in order to use it, all the individual runs should have been analyzed with the Triumph: Run pipeline 9.1 Handbook 9.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your folder has the right name and structure. Your input folder should have one subfolder per different run. Each of this subfolders contains the results of the Triumph: Run pipeline for that specific run. Please, do not rename any of the files produced by the Triumph: Run pipeline, otherwise they will not be found and cannot be included in the project report. Neither your folder nor your subfolders can have rare characters (only letters, numbers, dashes and underscores accepted). If your folder name contains different characters, it may not be recognized by the pipeline. IMPORTANT: Your folder name MUST NOT CONTAIN SPACES!!!! 9.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The triumph_microbiome_project pipeline can be found in this link. 9.1.3 Start the analysis. Basics Open the terminal. Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/triumph_project_report-master/ Exact name of downloaded folder might be slightly different depending on the version that was downloaded. Run the pipeline bash create_triumph_report.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/ The input directory (in this case /mnt/scratch_dir/my_folder) should be a folder containing one subfolder per run you want to include in the project report. These subfolders should contain the results of running the Triumph run pipeline using the --metadata option. Refer to the manual of that pipeline to see what that argument means. Note 1: If you are updating the project report because you added the analysis of an old run, you should first delete or rename the old report. This script might fail otherwise. Note 2: THE PROJECT REPORT PIPELINE ONLY WORKS WHEN ALL RUNS COLLECTED WERE EVALUATED WITH THE Triumph: Run pipeline USING A METADATA FILE!!! 9.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. If you prefer to store your output in a different folder, you can use the -o argument to give another path. bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ If you chose another site for the output, then you will find the same results there. In both cases, the output folder will contain the Triumph_Project_report.html file that is the actual report and a Triumph_Project_Report.log that contains messages, warnings and errors from R, where you can find more info if something goes wrong and the report could not be produced. 9.1.5 Troubleshooting Please read first the General Troubleshooting section! 9.1.5.1 Other problems Sometimes things fail. Before contacting me for help, please check the naming of your files, make sure you ran them with the Triumph_dada2_run_pipeline and that you did not change the name of any file. Try to create the report again. If you are sure all the input files are correct and the report does not run, please check the .log file to see if you can find the source of the error and contact me with that information. "]]
