[["index.html", "Bioinformatics Pipelines for Bacteriology (IDS, RIVM) 1 Introduction and General Instructions 1.1 General Instructions for all pipelines", " Bioinformatics Pipelines for Bacteriology (IDS, RIVM) Alejandra Hernandez-Segura 2022-05-10 1 Introduction and General Instructions In this book you will find the handbooks of all the available pipelines for the Bacteriology and Parasitology Department of the IDS (RIVM). This handbook is in continuous development, so please keep that in mind and contact Roxanne Wolthuis if you wish to add/change something or if you need help with troubleshooting (please read the troubleshooting section for each pipeline first). Important Note!! This handbook was written exclusively for people working at the RIVM (especially people at IDS). Some of the parameters or the way to run the pipeline would need to be changed for external people. Refer to the README section on the GitHub page of the corresponding pipeline. The pipelines that are enlisted as available through the internal GitLab from the RIVM are not accessible for external people. This might change in the future but for now that is the case. Equally, although the team would love to help external people to use our pipelines, we cannot give the personalized and fast-responding help that we give to people at the RIVM. If you do need help please write an Issue in the corresponding GitHub repository and we will try to help as soon as possible. 1.1 General Instructions for all pipelines 1.1.1 Setting up conda If you are not used to conda environments yet, you may not have your conda software completely set-up. At the RIVM conda is always pre-installed in our Linux environment (Wettenschappelijke werkplek) but not always set up. You need to go through this step once and hopefully never again. If you do not do this, you may have problems installing/running some of our pipelines. To set up conda to work properly in the Wettenschappelijke werkplek at the RIVM you need to follow these steps: Go to the scratch_dir partition and make a folder with your RIVM user name. Replace by your actual username (the RIVM one). cd /mnt/scratch_dir mkdir -p &lt;username&gt; Then you can go inside the folder you just created and make extra folders where your conda environments will be stored: cd /mnt/scratch_dir/&lt;username&gt;/ mkdir -p conda mkdir -p conda/envs mkdir -p conda/pkgs The next step is a bit more difficult. First you move back to your home directory, then you initiate conda and finally you give it the paths where you want it to make your environments (the folders you just made in scratch_dir): cd /home/&lt;username&gt;/ conda init bash touch /home/&lt;username&gt;/.condarc To add the paths you can do it from the command line using the vim .condarc editor if you are familiar with how vim works. If you dont know how to do that, you can simply go to your home directory by going to the menu Places &gt; Home on the top left of your Linux desktop. (#fig:find_home_dir)To find the home directory in RedHat Linux you need to go to the Menu Places and then to Home. There might be also a shortcut already on your desktop that you could follow as well. You can see some of the files/folders that you have on your desktop. The file that you want is called .condarc and it might not be visible yet. If you cannot see it, then you need to allow hidden files to be displayed. For that you can go to the menu (three parallel lines on the top right of the Places window) and make sure to have marked the Show Hidden Files option. You should now be able to see the .condarc file listed. (#fig:show_condarc)To find the .condarc file you first need to go to the menu of Places (1), then check the box to Show Hidden Files (2) and finally look for the .condarc file (3) Please open it by double-clicking the file name. A text editor should have opened. Finally, add or replace the following text with the correct paths (replace with your RIVM user name, meaning the folders you just made in scratch_dir): pkgs_dirs: - /mnt/scratch_dir/&lt;username&gt;/conda/pkgs envs_dirs: - /mnt/scratch_dir/&lt;username&gt;/conda/envs channel_priority: disabled Now you are ready to make your conda environments. 1.1.2 Requirements and preparation This handbook assumes that you are working at the wettenschappelijke werkplek environment at the RIVM. It is possible to run the pipeline in other settings and even on your laptop but you need extra steps that will not be enlisted here. Placing of the data: Your data should all be placed in one single folder (no subfolders) in the BioGrid (/data/BioGrid/&lt;my_folder&gt;/&lt;my_data&gt;/) or in the scratch_dir folder (/mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/). I strongly advice you to place it in the scratch_dir folder and to only copy it later to the BioGrid after the analysis. The run will be faster and therefore you will block the cluster less time for everyone else. Make sure that the folder that you use as input (the one that contains your input data) has the right name. This means: the folder that contains your data can have any name you want provided that you only use letters, numbers or underscores. If your folder name contains different characters, it may not be recognized by the pipeline. IMPORTANT: Your folder name and the names of every folder and subfolder in its PATH (so all the folders and subfolders that you have to enter to reach your input folder) should NOT CONTAIN SPACES!!!! Make sure that your files have the right format: if they are fastq files, they should have the extension (.fastq, .fq, .fastq.gz or .fq.gz). If they are fasta files, they should have the extension .fasta. Any other requirements in the input files will be specified in the section of that specific pipeline. 1.1.3 Downloading pipelines All the bacteriology pipelines created by the IDS-bioinformatics group are stored in either GitHub or the internal GitLab of the RIVM. Only people who belong to the RIVM and that are inside one of our servers/environments can access to the later one with their normal RIVM login details. If you are going to download a pipeline, please do so in the same partition that your data is (preferentially scratch_dir). Each pipeline handbook has the instructions on where to find the code (either GitHub or GitLab). You can download every pipeline through the website or through the command line: GitHub website Go to the website for the pipeline (check the section of the specific pipeline). Press the green button Code on the page and then click on Download zip (see Figure 1.1 for explanation). A zip file (-master.zip) will have likely be downloaded on your Downloads folder. Please move this zip file to the BioGrid or the scratch_dir partitions, depending on where your data is. Extract the files of the zip file. In Linux this is normally done by pressing the left button of the mouse, then Open with Archive Manager and then press Extract on the two windows that will consecutively appear. You could then delete the zip file (see Figure 1.2 for explanation). FIGURE 1.1: Pipelines can be downloaded directly from their GitHub website. FIGURE 1.2: Unzipping a repository in Linux. GitLab website Go to the website for the pipeline (check the section of the specific pipeline). Press the small white button with a cloud and a downwards arrow. In the drop-down menu, choose Download zip (see Figure 1.3 for explanation). A zip file (-master.zip) will have likely be downloaded on your Downloads folder. Please move this zip file to the BioGrid or the scratch_dir partitions, depending on where your data is. Extract the files of the zip file. In Linux this is normally done by pressing the left button of the mouse, then Open with Archive Manager and then press Extract on the two windows that will consecutively appear. You could then delete the zip file (see Figure 1.2 above for explanation). FIGURE 1.3: Pipelines can be downloaded directly from their GitLab website. Command-line Any member of the RIVM has a (RIVM-specific) GitLab account. You can log in with the same credentials that you use for accessing your workspaces. In the case of pipelines hosted on GitHub, you may need a free GitHub account. Open the terminal (you could also open terminator) by going to the Applications menu of the linux environment. FIGURE 1.4: Pipelines can be downloaded directly from their GitLab website. Go to the location where you want to download the pipeline using the command cd. For instance: cd /mnt/scratch_dir/&lt;my_folder&gt;/ Note: mind the slash at the beginning of the path Download the pipeline using the git clone command git clone &lt;url_to_the_pipeline&gt;.git Note: notice that I added .git to the URL of the pipeline. You will be asked to give your credentials (username + password) and then the (already unzipped) pipeline should have been downloaded in your current folder. 1.1.4 What to expect while running a Juno pipeline The Juno pipelines usually run easily with minimum input of the user required. Detailed information on the required input can be found in the section for the specific pipeline. Sometimes a pipeline can ask for input from the user to agree on installing software or a database (Figure 1.5 step 1), if you get asked to give permission, please do so in order to run the pipeline. This is most often in the form of pressing y followed by enter (Figure 1.5 step 2). The first time a Juno pipeline is executed, the preparation might take longer than expected. This is due to the installation of the basic software that is required for the pipeline to run. Be patient! You can recognize that the pipeline is still preparing when it shows a blinking box (Figure 1.5 step 3) or when it keeps printing lines inside the terminal. If the installation takes longer than 1 hour check the section General Troubleshooting. FIGURE 1.5: Screenshot of the Juno-typing pipeline in the terminal. The pipeline asks to create an environment(step 1). User input is required. To proceed the pipeline press y followed by enter. If executed correctly the pipeline will now show that it is creating the environment(step 2). The blinking box at the bottom indicates that the pipeline is still installing/updating(step 3). After installation the Juno pipeline will start running. Now the terminal shows yellow and/or green messages. These messages indicate that the pipeline is being executed (Figure 1.6). At the start of the yellow messages the number of steps is shown (Figure 1.6 step 1). The amount of steps indicate the progress of the pipeline in %. Once the pipeline is finished there will be a message printed that all the steps are performed (Figure 1.6 step 2). FIGURE 1.6: Screenshot of the Juno AMR pipeline in the terminal. The pipeline is executing all required jobs to create the requested results. The terminal shows a list of all the jobs that will be executed(step 1). Furthermore there is green and yellow text that shows the steps of the pipeline. At the bottom of the terminal there is a line that shows the pipeline is finished(step 2). If a step fails, you will likely see red text appearing on the screen (Figure 1.7). The pipeline might proceed the other steps. If the pipeline fails check the section General Troubleshooting for more help or any other problems you may encounter. FIGURE 1.7: Screenshot of the Juno AMR pipeline in the terminal. The pipeline failed on one of the steps. The red text shows an error message. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max. With sequencing data, the storage space can get full quite fast. Note2: In this manual, a couple of non-Juno pipelines are included. They work mostly in the same way but the differences are in the design and maintenance (the Juno pipelines are named like that and they are very actively maintained, the others less so). 1.1.5 General output and log files for every pipeline Every pipeline (with few exceptions) generates some extra output that is not essential for your analysis but that it is required for debugging/troubleshooting. Especially, if you are contacting someone at the IDS-bioinformatics team, we expect you to have these files at hand or to also send the location where they can be found (make sure that we have right to access that location or move them to a shared folder where they can be accessed). The log/ subfolder is located inside your output directory. It contains one file per every step performed by the pipeline for each sample. There you can find error messages or some information of what happened during each step. The messages are not always easy to interpret, but they often have clues on why a job/analysis failed. Sometimes the log files for each tool (and sample) are empty because either, there were no problems or messages generated on the run or because the problem lies before the job/analysis was even started. You can look at them of course, but it is ok if you do not understand the messages. However, for us as bioinformaticians it is essential to have access to these files when something goes wrong inside one of the scripts/tools used. The log/cluster/ or log/drmaa/ subfolder is also inside the output directory and inside the log subdirectory. Here you can find other type of logging files of any job performed by the pipeline. This is even more technical logging but it can be veru useful to debug system errors, problems with memory, etc. The audit_trail or results subfolders are also located inside the output directory. They contain contain 4 very important files for trace-ability of your samples. The log_conda.txt file contains information about the software that was necessary and that was contained in your environment. This means basically the software that would be needed to reproduce the same circumstances in which the pipeline was run and how it can be reproduced. The log_config.txt file is even more informative. It enlists all the parameters used to run the pipeline. In case months later you forgot how you got the results you did or you just want to know some details about the analyses, they are all stored there. The log_git.txt has information about the repository or the code that was downloaded. It tells you exactly how it was downloaded so you can reproduce it at a later time point. Finally, the snakemake_report.html has a nice overview of the different steps that were performed with your samples, when were they performed, which output was produced and which software was used, as well as some statistics on how the run went. Even though you would not look at this output regularly, it is useful for us as bioinformaticians to be able to help you if a problem would arise. However, they are also useful for you! Especially the audit_trail has the information you would need to put in a publication (software versions, parameters, etc.). Please do not delete them and know they are important. "],["general-troubleshooting.html", "2 General Troubleshooting 2.1 Solving problems when installing a pipeline or activating a pipeline environment 2.2 Failure when installing master environment 2.3 Failure to make a sample sheet or to find input directory 2.4 Other problems or failing rules", " 2 General Troubleshooting body { text-align: justify} Important Note!! This handbook was written exclusively for people working at the RIVM (especially people at IDS). Some of the parameters or the way to run the pipeline would need to be changed for external people. Refer to the README section on the GitHub page of the corresponding pipeline. The pipelines that are enlisted as available through the internal GitLab from the RIVM are not accessible for external people. This might change in the future but for now that is the case. Equally, although the team would love to help external people to use our pipelines, we cannot give the personalized and fast-responding help that we give to people at the RIVM. If you do need help please write an Issue in the corresponding GitHub repository and we will try to help as soon as possible. 2.1 Solving problems when installing a pipeline or activating a pipeline environment You should follow the instructions to set up conda before using any of our pipelines. If you did not do that, you may run into some issues and cryptic error messages. One of the most common error messages happens when you try to do conda activate &lt;pipeline_env&gt; where is the name of the environment especific to the pipeline you are using. The error message will say something like this: CommandNotFoundError: Your shell has not been properly configured to use &#39;conda activate&#39;. To initialize your shell, run $ conda init &lt;SHELL_NAME&gt; First you need to follow the instructions to set up conda and try installing the pipeline again. However, if you had already tried making some environments before following the conda configuration steps, your issue might not be solved. Specifically, when you try to use conda activate &lt;env_name&gt; you may get this error message now: Could not find conda environment: &lt;env_name&gt; You can list all discoverable environments with `conda info --envs`. It is likely that you already made a conda environment but in the wrong place and conda cannot make the right one without you deleting the old (misplaced) one. In that case we need to find the wrong one and delete it. For that please type: conda env list This command will show you a list of your current environments. The result in the command line will look something like this: # conda environments: # base * /mnt/miniconda /home/&lt;username&gt;/.conda/envs/&lt;env_pipeline&gt; mamba /home/&lt;username&gt;/.conda/envs/mamba You probably can notice that the paths that are in the second column do not correspond to what you would expect as explained in the instructions to set up conda. Instead, some of your environments were made in the /home/&lt;username&gt;/.conda folder and that is precisely the one we need to delete. Note: DO NOT DO ANYTHING WITH THE base environment. That one is always placed in a different path. To delete the wrong environments just type: rm -rf /home/&lt;username&gt;/.conda Note: Replace /home/&lt;username&gt;/.conda by the conda folder where the environments were meade. MAKE SURE TO INCLUDE THE FULL PATH NAME UP TO THE .conda FOLDER, OTHERWISE YOU COULD BE DELETING ESSENTIAL FOLDERS IN YOUR DESKTOP. Please ask a bioinformatician for help if you are unsure about how to do this step. Note2: You must realize that this will delete ALL your previous conda environments (also the ones unrelated to the Juno pipelines. This step might take a while because the folder usually contains many small files. However, after this step is done, you can try to install the pipeline again (making the new environments) according to the instructions of the pipeline you are using. If everything went fine, the conda activate &lt;pipeline_env&gt; command should work fine now. If you wonder how do you know whether this command worked or not, you can just look at your command line. Before activating an environment it should look like this: (base) [hernanda@rivm-biohn-l05p rondzendingen]$ Look at the word (base) at the beginning of the line. After running the conda activate &lt;pipeline_env&gt; command it should look like this: (&lt;pipeline_env&gt;) [hernanda@rivm-biohn-l05p rondzendingen]$ The name of the environment you just activated should now appear at the beginning of the line (&lt;pipeline_env&gt;). Also, when you do conda env list you should see the correct paths (/mnt/scratch_dir/&lt;username&gt;/conda) for your environments: # conda environments: # base * /mnt/miniconda &lt;env_pipeline&gt; /mnt/scratch_dir/&lt;username&gt;/conda/envs/&lt;env_pipeline&gt; mamba /mnt/scratch_dir/&lt;username&gt;/conda/envs/mamba 2.2 Failure when installing master environment Sometimes the installation does not work because of the settings of Linux. The installation always takes some minutes in which a message saying Solving environment... might appear on the screen. However, if this step takes too long (for instance 30 min or more) please abort the step by pressing Ctrl + C and/or Ctrl + Z (sometimes you need to press repeateadly). In such cases, please change the necessary settings with this command: conda config --set channel_priority false Then try to run the pipeline and install the master environment again. 2.3 Failure to make a sample sheet or to find input directory There are few reasons why this could happen: Your path (all the folders that lead to your input directory) contains spaces or unrecognized characters. You mis-spelled something in your path or gave the wrong path. Especially while working in Linux, it is common to forget a slash (/) at the beginning of a full path. For instance: /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/. Your files might have the incorrect format and/or extension. Normally for all the files, standard names are accepted (for instance, .fastq.gz or .fastq for raw reads and .fasta for fasta files). Please make sure that your files have the correct and classical extensions. Your input directory does contain the expected files, but they are placed in sub-folders. Most pipelines expect all input files to be together in one folder instead of scattered in sub-folders. The only exception is when you use output from a previous pipeline (for instance, Juno-assembly) into another pipeline. Please collect all your samples together in one folder. 2.4 Other problems or failing rules The Juno pipelines are still in development which means that sometimes things fail. Before contacting me for help, please try these steps: Re-run the pipeline again and see if it goes further. If it does, please keep re-running the pipeline until your analysis is finished or it just doesnt go further. Even if you are able to finish your analysis, just send me an email afterwards (see step 3) so I can check what happened. Download the pipeline again and start from the beginning of this handbook. Sometimes the issue has been resolved in newer versions of the pipeline. Collect your logging files and contact me. Please inform me about bugs/errors via e-mail sending also your log files and the path where I can find your input directory and the pipeline. No screenshots are necessary. Note that if you do not send this information, I will not be able to help you and your and my work will be delayed. "],["juno-assembly.html", "3 Juno-assembly pipeline 3.1 Handbook", " 3 Juno-assembly pipeline body { text-align: justify} The goal of this pipeline is to generate assemblies from raw fastq files. The input of the pipeline is raw Illumina paired-end data in the form of two fastq files (with extension .fastq, .fastq.gz, .fq or .fq.gz), containing the forward and the reversed reads (_R1 and _R2 must be part of the file name, respectively). On the basis of the generated genome assemblies, low quality and contaminated samples can be excluded for downstream analysis. The pipeline uses the following tools: FastQC (Andrews, 2010) is used to assess the quality of Illumina reads. In this pipeline, it is used before and after trimming/filtering FastP (Chen, Zhou, Chen and Gu, 2018) is used to remove poor quality data and adapter sequences Picard determines the library fragment lengths The reads are assembled into scaffolds by SPAdes (Bankevich et al., 2012) by means of de novo assembly of the genome. SPAdes uses k-mers for building an initial de Bruijn graph and on following stages it performs graph-theoretical operations to assemble the genome. Kmer sizes of 21, 33, 55, 77 and 99 were used. For de novo assembly, SPAdes isolate mode is used QUAST (Gurevich, Saveliev, Vyahhi, &amp; Tesler, 2013) is used to assess the quality of the filtered scaffolds To assess the quality of the microbial genomes, CheckM (Parks, Imelfort, Skennerton, Hugenholtz, &amp; Tyson, 2015) is used. CheckM calculates scores for completeness, contamination and strain heterogeneity Bbtools (Bushnell, 2014) is used to generate scaffold alignment metrics MultiQC (Ewels, Magnusson, Lundin, &amp; Käller, 2016) is used to summarize analysis results and quality assessments in a single report for dynamic visualization. Kraken2 and Bracken for identification of bacterial species. 3.1 Handbook 3.1.1 Requirements and preparation See the General Instructions for all pipelines first. You have different possibilities to run the complete pipeline. Your input directory should contain samples with just ONE genus type. If this is the case, you should tell the pipeline which genus you have (e.g. using the flag --genus Salmonella). Read further for more details. You can provide a metadata CSV file that should contain at least two columns: sample and genus (all in small letters). . If this is not the case DO NOT MODIFY THAT EXCEL FILE! Instead follow the steps mentioned in the Troubleshooting section, particularly in the part Error saying that my samples cannot be found on the Excel file with the genus list If you do not do any of the above, the pipeline will run just fine. The only difference is that the genome completeness will be calculated based on the genus identified in your sample (the one with most confidence) instead of based on the genus you provided. 3.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! . MANY PEOPLE CONTACT US WITHOUT HAVING DONE THIS STEP FIRST, SO MAKE SURE YOU CLICK IN THE INSTRUCTIONS TO SET UP CONDA AND FOLLOW THEM. If you have updated from Juno1.0 to 2.0, you have to do it! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-assembly pipeline can be found in this link: Juno-assembly pipeline. After downloading the pipeline, it needs to be installed. To install the pipeline, first enter the folder where you downloaded the pipeline and run the installation script: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_pipeline bash install_juno_assembly.sh 3.1.3 Start the analysis. Basics Open a terminal. (Applications &gt; terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_pipeline MAKE SURE YOU HAVE RUN THE INSTALLATION INSTRUCTIONS BEFORE USING THE PIPELINE FOR THE FIRST TIME OR AFTER AN UPDATE! Activate the environment that has the necessary software for the pipeline to work: conda activate juno_assembly If you run in trouble please see the troubleshooting section for conda activate. Run the pipeline If all your samples have the same genus, for instance, Salmonella, you run it like this: python juno_assembly.py -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --genus Salmonella Note that the genus should be ONE word. Do not put any species names! If you want to provide a metadata file do this: python juno_assembly.py -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --metadata &lt;path_to_file&gt;.csv If you do not want to provide the genus yourself, you can simply do : python juno_assembly.py -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. Optionally, deactivate the juno_assembly environment. conda deactivate 3.1.4 Output A folder called output/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per result (qc_raw_fastq, clean_fastq, qc_clean_fastq, de_novo_assembly, de_novo_assembly_filtered, qc_de_novo_assembly, identify_species). Please refer to the manuals of every tool to interpret the results. . Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) python juno_assembly.py -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later time point and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 3.1.5 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 3.1.5.1 Error saying that the genus supplied was not recognized by CheckM If you get this message: The genus &lt;your_provided_genus&gt; is not supported. it means that although you provided the genus information, it is probably not supported by the tool CheckM. First check that you have no spelling mistakes in the genus name of your sample(s) or that you used the correct case (first letter capital and the rest on small letters). If you are sure that your spelling is correct, it may be that CheckM does not have the genus you supplied on its database. As the message says, if you are not sure on how to spell the genus name or which genus is accepted, please check the list of accepted genera by typing. python juno_assembly.py -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --help-genera 3.1.5.2 Other problems or failing rules The Juno-assembly pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be a delay in troubleshooting the problem. "],["juno-typing.html", "4 Juno-typing 4.1 Handbook", " 4 Juno-typing body { text-align: justify} The goal of this pipeline is to perform bacterial typing (7-locus MLST and serotyping). It takes 2 types of files per sample as input: Two .fastq files (paired-end sequencing) derived from short-read sequencing. They should be already filtered and trimmed (for instance, with the Juno-pipeline). An assembly from the same sample in the form of a single .fasta file. Importantly, the Juno-typing pipeline works directly on output generated from the Juno-assembly pipeline. The Juno-typing pipeline will then perform the following steps: The appropriate 7-locus MLST schema and eventually a serotyper. The supported species for the 7-locus MLST can be found in the database generated by the Center for Genomic Epidemiology from the Technical University of Denmark. 7-locus MLST by using the MLST tool. If appropriate for the genus/species, the samples will be serotyped. The currently supported species are: Salmonella serotyper by using the SeqSero2 tool. E. coli serotyper by using the SerotypeFinder tool. S. pneumoniae serotyper by using the Seroba tool. Shigella serotyper by using the ShigaTyper tool. Disclaimer!! Importantly, you can provide a species while calling the pipeline and this will be considered as the species for ALL samples. Alternatively, you can provide a metadata file (explained below) with a different species per sample. The species is used to choose MLST scheme and serotyper. If you are using the results of the Juno-assembly as input and you do not provide another metadata file, the results of the species identification step in Juno-assembly will be used! Note that that step might not always be correct so you have to check that the serotyping and the MLST schema were properly chosen. 4.1 Handbook 4.1.1 Requirements and preparation See the General Instructions for all pipelines first. This pipeline needs two fastq file (R1 and R2) and an assembly (.fasta) files per sample. The fastq files should have been trimmed and filtered to remove low quality reads/bases. You could use the Juno-assembly pipeline for that. Moreover, that pipeline also provides the de novo assembly for your samples and the output folder of the Juno-assembly pipeline can be used directly into the Juno-typing pipeline. If you, however, prefer to use any other tool for doing your assembly and trimming/filtering, make sure that the fastq files and fasta files have the same name (for instance, sample1_R1_001.fastq.gz, sample1_R1_001.fastq.gz and sample1.fasta). If that is not the case, the files may not be recognized as belonging to the same sample. Also, ALL THREE FILES SHOULD BE IN THE SAME FOLDER! If you have multiple samples, they should all be in the same input folder, NOT IN SUBFOLDERS. The only exception is if you use the Juno-assembly pipeline to pre-process your data. In that case, the pipeline will recognize the subfolders where the fastq files and the fasta files should be. 4.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-typing pipeline can be found in this link. 4.1.3 Install conda environment YOU NEED TO REINSTALL THE MASTER ENVIRONMENT EVERY TIME YOU UPDATE THE PIPELINE (everytime you download the code) Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-typing If you already had a juno_typing environment before you need to delete the old one by using the command: conda env remove -n juno_typing If you had never created a juno_typing environment before, you can skip this step and go to step 4 instead. Create a new environment for running Juno_typing by using the command: conda env create -f envs/master_env.yaml This step will take some time (few minutes). Note: If this step would take more than 1 hour, please kill the process (using Ctrl + C or Ctrl + Z) and refer to the section General Troubleshooting. The first issue written there (Failure when installing master environment) often solves the problem. If, however, the problem persists, please contact me by email. 4.1.4 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-typing Activate juno_typing environment conda activate juno_typing If you run in trouble please see the troubleshooting section for conda activate. Run the pipeline This can be done in three ways. The first one is just providing an input directory with the results of the juno_assembly pipeline: python juno-typing -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;results_juno_assembly&gt;/ The second one is providing an input directory as well as a metadata (csv) file. This file should contain at least one column with the sample name (name of the file but removing [_S##]_R1.fastq.gz), a column called genus and a column called species. If a genus + species is provided for a sample, it will overwrite the species identification performed by this pipeline when choosing the scheme for MLST and the serotyper. Example metadata file: sample genus species sample1 Salmonella enterica python juno_typing.py -i my_input_files --metadata path/to/my/metadata.csv The last way is to tell the pipeline which species the samples have. Note that only ONE species can be given for ALL the samples, so it will be assumed that they all belong to the same one. Each species should have two words (genus + species). python juno_typing.py -i my_input_files --species salmonella enterica If you give both, a metadata file and a --species, the --species will take precedence and overwrite the metadata file. Note: The fastq files corresponding to this sample would probably be something like sample1_S1_R1_0001.fastq.gz and sample2_S1_R1_0001.fastq.gz and the fasta file sample1.fasta. Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 4.1.5 Output A folder called output/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per step (mlst7 and serotype). Please refer to the manuals of every tool to interpret the results. In each one of these folders, there should be a sub-folder per sample and, for the case of mlst7 and serotype, also a csv file collecting the results of all the samples together: a serotype/serotype_multireport.csv and mlst7/mlst7_multireport.csv. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) python juno_typing.py -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later time point and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 4.1.6 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 4.1.6.1 Other problems or failing rules The Juno-typing pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be a delay in troubleshooting the problem. "],["juno-amr.html", "5 Juno-amr 5.1 Handbook", " 5 Juno-amr body { text-align: justify} The Juno Antimicrobial Resistance(Juno AMR) pipeline is a pipeline that is used to automate multiple tools that help identify acquired genes and find chromosomal mutations mediating antimicrobial resistance in DNA sequences of bacteria. The tool takes paired-end .fastq files as input. The input files can contain full or partial sequences. The output can be used in antimicrobial resistance analysis and is combined in four summary files for a quick overview of the most important results. The tools that are being used in this pipeline are listed below: ResFinder is a tool that identifies acquired antimicrobial resistance genes based on databases. The tool is created by The Center For Genomic Epidemiology. PointFinder is a tool that searches for chromosomal point mutations that mediate resistance to select antimicrobial agents for some bacterial species. The tool is created by The Center For Genomic Epidemiology. 5.1 Handbook 5.1.1 Requirements and preparation See the General Instructions for all pipelines first. To run this pipeline the type of species and a folder with paired .fastq files is required. Place all your samples in one folder, no subfolders, all file types should be the same format. Besides giving input it is also required to select a species. The --species parameter is required for PointFinder in order to find the chromosomal mutations. If you dont know the species you can fill this parameter with other. This way the pipeline will run ResFinder without running PointFinder. The output of the pipeline will be collected in the folder output within the working directory (the directory where the pipeline is active). If you want the output to be in a different location it is possible to do this with the --output parameter. Note: all samples in the directory should be the same species and the same file format. 5.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-amr pipeline can be found here. 5.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-amr Run the pipeline python3 juno-amr.py -s [species] -i [/mnt/scratch_dir/&lt;my_folder&gt;/fastq_file folder] Please read the section What to expect while running a Juno pipeline. See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You can add the --output parameter to the command in order to place your output somewhere else. 5.1.4 Output In the output you can find 3 folders: log: Log with output and error file from the cluster for each snakemake rule/step that is performed. results_per_sample: Output produced by ResFinder and PointFinder for each sample. CGEs explanation on the output can be found here. summary: Directory with 4 summary files created from each sample within the results_per_sample folder. summary_amr_genes.csv: Shows the samplename resistance gene, identity, alignment length/gene length and coverage per gene hit. summary_amr_phenotype.csv: Shows the type of match for each sample on the resistance for each antimicrobial. summary_amr_pointfinder_predicition.csv: - Shows the samplename and a 0(no hit) or 1(hit) prediciton for each mutation. summary_amr_pointfinder_results Shows the samplename, mutation, nucleotide change, amino acid change, resistance and PMID for each mutation hit Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o or --output python3 juno-amr.py -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -species salmonella -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ 5.1.5 Troubleshooting for this pipeline Please read the General Troubleshooting section first! 5.1.5.1 Other problems or failing rules The Juno-amr pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be a delay in troubleshooting the problem. "],["juno-cgmlst.html", "6 Juno_cgMLST 6.1 Handbook", " 6 Juno_cgMLST body { text-align: justify} The goal of this pipeline is to perform cgMLST on bacterial genomes. As input, it requires an assembly for each sample you want to analyze in the form of a single .fasta file. Importantly, this pipeline works directly on output generated from the Juno-assembly pipeline. The Juno-cgMLST uses ChewBBACA to find the cgMLST profile of the given genomes. Besides, the result table with the allele numbers is translated to a table with (sha1) hashes so that they can be shared and compared even if each result used a different database. Note: We highly encourage to use the hashes for the analysis instead of the allele numbers. This will make your data more reproducible in case there was an update in the database that assigns allele numbers or to share it with other colleagues. 6.1 Handbook 6.1.1 Requirements and preparation See the General Instructions for all pipelines first. This pipeline needs one .fasta file (the assembly) per sample as input. ALL THE INPUT FILES SHOULD BE IN THE SAME FOLDER, NOT IN SUBFOLDERS. The only exception is if you use the Juno-assembly pipeline to pre-process your data. In that case, the pipeline will recognize the subfolders where the assemblies should be. 6.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-cgMLST pipeline can be found in this link. 6.1.3 Install conda environment YOU NEED TO REINSTALL THE MASTER ENVIRONMENT EVERY TIME YOU UPDATE THE PIPELINE (everytime you download the code) Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_cgmlst If you already had a juno_typing environment before you need to delete the old one by using the command: conda env remove -n juno_cgmlst If you had never created a juno_typing environment before, you can skip this step and go to step 4 instead. Create a new environment for running Juno_typing by using the command: conda env create -f envs/master_env.yaml This step will take some time (few minutes). Note: If this step would take more than 1 hour, please kill the process (using Ctrl + C or Ctrl + Z) and refer to the section General Troubleshooting. The first issue written there (Failure when installing master environment) often solves the problem. If, however, the problem persists, please contact me by email. 6.1.4 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_cgmlst Activate juno_cgmlst environment conda activate juno_cgmlst If you run in trouble please see the troubleshooting section for conda activate. Run the pipeline This can be done in three ways. The first one is just providing an input directory with the results of the juno_assembly pipeline: python juno-cgmlst -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;results_juno_assembly&gt;/ The second one is providing an input directory as well as a metadata (csv) file. This file should contain at least one column with the sample name (name of the file but removing [_S##]_R1.fastq.gz) and a column called genus. If a genus is provided for a sample, it will overwrite the species identification performed by this pipeline when choosing the scheme for MLST and the serotyper. Example metadata file: sample genus species sample1 Salmonella enterica Note: The fastq files corresponding to this sample would probably be something like sample1_S1_R1_0001.fastq.gz and sample2_S1_R1_0001.fastq.gz and the fasta file sample1.fasta. python juno_cgmlst.py -i my_input_files --metadata path/to/my/metadata.csv The last way is to tell the pipeline which genus the samples have. Note that only ONE genus can be given for ALL the samples, so it will be assumed that they all belong to the same one. python juno_cgmlst.py -i my_input_files --genus salmonella If you give both, a metadata file and a --genus, the --genus will take precedence and overwrite the metadata file. Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 6.1.5 Output A folder called output/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. Please refer to the manual of ChewBBACA to interpret the results. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) python juno_cgmlst.py -i juno_cgmlst.py -i my_input_files --metadata path/to/my/metadata.csv -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later time point and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 6.1.6 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 6.1.6.1 Other problems or failing rules The Juno-cgMLST pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be a delay in troubleshooting the problem. "],["juno-annotation.html", "7 Juno-annotation 7.1 Handbook", " 7 Juno-annotation body { text-align: justify} This pipeline takes assemblies (.fasta) as input and performs gene annotation. It should be able to annotate bacterial genomes and bacterial plasmids. The pipeline follows these steps: Filtering contigs with more than 200bp. This step is necessary for downstream tools to run properly and, in general, small contigs are often filtered to reduce the noise of possible contamination. Besides, these small contigs often do not contain much valuable information. Finding the start of the chromosome ( Circlator ). Although in most cases, the chromosome is already circularized after the assembly, in the cases in which that was not possible, this step might help. The start of the chromosome will be set at the beginning of the dnaA gene. For the plasmids or other smaller contigs, the start of a predicted gene near its center is used. Annotation using Prokka. Prokka is a fast program for prokaryotic genome annotation. We enriched the databases that Prokka relies on by using the RefSeq database. This step is very slow if run locally (~ 3 hours) but in the cluster it takes around 30 min per sample. 7.1 Handbook 7.1.1 Requirements and preparation See the General Instructions for all pipelines first. The pipeline needs to know the Genus and Species of each input fasta file. There are three ways to do this (see the section Run the pipeline for more details on how to do this and the recognized abbreviations): The pipeline can guess this information from the file name provided the appropriate abbreviations are used within the name. You can provide a metadata file (.csv) that should contain at least three columns: File_name, Genus and Species (mind the capital letters). The File name is case sensitive, so the names of the files (without the full path) should coincide EXACTLY with your input fasta files. The genus and species should be recognized as an official TaxID. You MUST write the genus and the species in the appropriate column, never together in one column. This option cannot be combined with the first one. This means that if you decide to guess the genus and species from the file name, the provided metadata file will be ignored. You can provide the genus and species directly when calling the pipeline. The genus and species should be recognized as an official TaxID. YOU CAN ONLY PROVIDE ONE GENUS AND ONE SPECIES and this will be used for all samples. You can combine this option with one of the two above. This means that if you have multiple samples but some of them are not enlisted in your metadata file, they will instead inherit the genus and species from the information provided directly when calling the pipeline. If all the files were enlisted in the metadata, this option will be ignored. 7.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno-annotation pipeline can be found in this link. 7.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno-annotation Run the pipeline If all your samples have the same genus and species, for instance, Klebsiella pneumoniae, you run it like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --genus Klebsiella --species pneumoniae Note that the genus and species should be ONE word. Do not put genus and species together! If your samples do not have all the same genus, you can make a metadata file. This file should be a .csv file with at least these columns and information: File_name Genus Species sample1_Kpn.fasta Klebsiella pneumoniae sample2Pae2.fasta Pseudomonas aeruginosa sample3Sau_1.fasta Staphylococcus aureus Note that the name of the columns should be EXACTLY the same than in this example, including the underscores and the capital letters. You would then call the pipeline like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --metadata path/to/metadata.csv Alternatively, if your input files have one of the following abbreviations somewhere in the data, the genus and species may be automatically guessed from the file name: Abbreviation Genus Species Cam Citrobacter amalonaticus Cbr Citrobacter braakii Cfr Citrobacter freundii Cse Citrobacter sedlakii Cwe Citrobacter werkmanii Ebu Enterobacter bugandensis Eca Enterobacter cancerogenus Ecl Enterobacter cloacae Eco Escherichia coli Exi Enterobacter cloacae Kae Klebsiella aerogenes Kox Klebsiella oxytoca Kpn Klebsiella pneumoniae Mmo Morganella morganii Pae Pseudomonas aeruginosa Pmi Proteus mirabilis Rpl Raoultella planticola Sar Staphylococcus argenteus Sau Staphylococcus aureus Sma Serratia marcescens Then you would call the pipeline like this: bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ --make-metadata Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 7.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (circlator, filtered_contigs and prokka). Please refer to the manuals of every tool to interpret the results. Your main result will be two genebank (.gbk) files per sample that contained your annotated genomes/plasmids. You can find them inside the subfolders out/prokka. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (from output) bash start_annotation.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 7.1.5 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 7.1.5.1 Failure to find metadata file You could get an error associated to the metadata: The provided species file &lt;your_metadata_file.csv&gt; does not exist. Please provide an existing file &quot;If you used the option --make-metadata, please check that all the fasta files contain the .fasta extension and that the file names have the right abbreviations for genus/species This message means that either you provided the wrong path/name to your metadata file, that this has the wrong extension (not .csv) or that, if you used the option to --make-metadata, your input files do not have the right abbreviations. Please check the metadata file provided or the file names and try again. 7.1.5.2 Other problems or failing rules The Juno-annotation pipeline is still in development which means that sometimes the process can fail. This pipeline is not very actively maintained but if you run into problems, you can ask Roxanne Wolthuis or Fabian Landman. "],["juno-blast.html", "8 Juno_blast 8.1 Handbook", " 8 Juno_blast body { text-align: justify} The goal of this pipeline is to perform BLAST in the input file(s) contained in the input directory. The input file should be a (multi) fasta file. 8.1 Handbook 8.1.1 Requirements and preparation See the General Instructions for all pipelines first. This pipeline requires one .fasta file per sample. Note that the input files MUST have the extension .fasta. An output file will be created inside the output directory with the sample name as prefix. That file should have your results for BLAST. Note that if your fasta file has more than one sequence, the output of all of them will be included in the result file. 8.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Juno_blast pipeline can be found in this link. 8.1.3 Install conda environment YOU NEED TO REINSTALL THE MASTER ENVIRONMENT EVERY TIME YOU UPDATE THE PIPELINE (everytime you download the code) Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_blast If you already had a juno_blast environment before you need to delete the old one by using the command: conda env remove -n juno_blast If you had never created a juno_blast environment before, you can skip this step and go to step 4 instead. Create a new environment for running Juno_blast by using the command: conda env create -f envs/master_env.yaml This step will take some time (few minutes). Note: If this step would take more than 1 hour, please kill the process (using Ctrl + C or Ctrl + Z) and refer to the section General Troubleshooting. The first issue written there (Failure when installing master environment) often solves the problem. If, however, the problem persists, please contact me by email. 8.1.4 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline using: cd /mnt/scratch_dir/&lt;my_folder&gt;/Juno_blast Activate juno_blast environment conda activate juno_blast Run the pipeline by providing an input directory: python juno_blast -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ Please read the section What to expect while running a Juno pipeline. See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 8.1.5 Output A folder called output/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) python juno_blast -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later time point and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 8.1.6 Troubleshooting for this pipeline Please read first the General Troubleshooting section! 8.1.6.1 Other problems or failing rules The Juno_blast pipeline is still in development which means that sometimes the process can fail. Before contacting for help, try these two steps: Re-run the pipeline again and see if the process continues. If it does, please keep re-running the pipeline until your analysis is finished or there is no longer progress. In this case, send an email after the pipeline is finished so I can troubleshoot the problem. Download the pipeline again and start from the beginning of this handbook. Sometimes there is an issue that has been resolved in newer versions of the pipeline. If the pipeline still fails after these two steps, please inform me about the problem. Send an e-mail with the following content: The log and error files that can be found in the output folder The path to your input directory The path to where the pipeline is installed Note: I cannot help you without this information, if information is missing there will be a delay in troubleshooting the problem. "],["Myco-lofreq.html", "9 Low frequency variants in Mycobacterium samples 9.1 Handbook", " 9 Low frequency variants in Mycobacterium samples CURRENTLY NOT MAINTAINED OR SUPPORTED body { text-align: justify} The main purpose of this pipeline is to call minority variants in Mycobacterium samples. I takes paired-end raw fastq files as input (one should contain R1 and the other R2 on the name). The pipeline performs the following steps: Quality Control of the raw reads using FastQC Trimming with Trimmomatic Quality Control of the trimmed reads using FastQC Alignment to reference genome using BWA Preparation to load in LoFreq Calling SNP and indels using LoFreq Annotating the resulting vcf file using vcf-annotator Although the pipeline has been written for Mycobacterium, it can easily be extended to other type of bacteria. Please contact me if you want to use the pipeline for other organism. 9.1 Handbook 9.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your files have the right format: they must have a fastq extension (.fastq, .fq, .fastq.gz or .fq.gz) and contain the characters R1 (for forward reads) or R2 (for reverse reads) somewhere on the name. As with the folder name, you should avoid rare characters on your file names. Just use letters, numbers, underscores or dashes. Make sure there are no spaces on the file names. 9.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Myco-lofreq pipeline can be found in this link. 9.1.3 Start the analysis. Basics Open the terminal. You can go to the Linux menu called Applications and open the program terminal or the terminator one. Both should work. Enter the folder of the pipeline cd /mnt/scratch_dir/&lt;my_folder&gt;/Myco_lofreq Run the pipeline bash run_myco_lofreq_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 9.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (fastqc, trimmomatic, multiqc, bwa_alignment, lofreq, etc). Please refer to the manuals of every tool to interpret the results. There are two important subfolders generated by this pipeline: A folder called out/, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (kmerfinder, mlst7 and serotype). Please refer to the manuals of every tool to interpret the results. Each one of these folders, there should be a sub-folder per sample and, for the case of mlst7 and serotype, also a csv file collecting the results of all the samples together: a serotype/salmonella_serotype_multireport.csv, serotype/ecoli_serotype_multireport.csv and mlst7/mlst7_multireport.csv. Even if your samples are not Salmonella or E. coli you will get the multireport file, althought it will be empty. Although the results of kmerfinder are provided, these have not been validated. They are used only to choose the right scheme for the MLST and the right serotyper. If you would use the results of kmerfinder as a species identification tool, you do it under your own risk and you should be able to interpret correctly the results. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash run_myco_lofreq_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 9.1.5 Troubleshooting Please read first the General Troubleshooting section! 9.1.5.1 Other problems or failing rules The Myco-lofreq pipeline is still in development which means that sometimes the process can fail. This pipeline is not being maintained anymore so at the moment we cannot provide support for it. "],["juno-deinterleave.html", "10 Juno-deinterleave 10.1 Handbook", " 10 Juno-deinterleave CURRENTLY NOT MAINTAINED OR SUPPORTED body { text-align: justify} # Parameters parameters &lt;- list(&quot;pipeline_name&quot; = &quot;Deinterleave&quot;) 10.1 Handbook 10.1.1 Requirements and preparation See the General Instructions for all pipelines first. 10.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The Myco-lofreq pipeline can be found in this link. 10.1.3 Start the analysis. Basics Open a terminal. (Applications&gt;terminal). Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/deinterleave Run the deinterleave script bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz 10.1.4 Output Your two deinterleaved files will be located in the current directory (you can check your current directory by running the command pwd) and they will have the same name than the original file, but with the suffix \"_R1\" for the forward reads and \"_R2\" for the reverse reads. For example: Original file: sampleX.fastq Deinterleaved files: Forward: sampleX_R1.fastq Reverse: sampleX_R2.fastq Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (from output) bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz -o /mnt/scratch_dir/my_results 10.1.5 Extra options Choose name of deinterleaved files You can also use the names you want for the output files. For that you can do: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz --output_r1 my_deinterleaved_file_R1.fastq --output_r2 my_deinterleaved_file_R2.fastq Note that I did not use paths there. Your output directory should be set with the option -o Compress output You may want to have the deinterleaved files compressed. Note that the compression might take a bit long. For that you can do: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz --compress That will make sure that your two interleaved files are gzipped (and have the extension .fastq.gz). Combine options It is possible to combine more than one of this options. For example: bash deinterleave.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_interleaved_file&gt;.fastq.gz -o /mnt/scratch_dir/my_results --compress "],["triumph-run.html", "11 Triumph: Run pipeline 11.1 Handbook", " 11 Triumph: Run pipeline CURRENTLY NOT MAINTAINED OR SUPPORTED body { text-align: justify} This pipeline processes raw fastq files from microbiome data using the Dada2 R-package. At the end, it also produces a report summary of the run. The pipeline has been tailor-made for the microbiome group at the RIVM, especifically for the Pienter project. It therefore expects the input that is normally generated there and might not be very flexible with sample names and metadata. I will try to explain what is expected from the input when necessary. 11.1 Handbook 11.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your files have the right format: they must have a fastq extension (.fastq, .fq, .fastq.gz or .fq.gz) and have specific names. The names should be only numbers (they can be preceded by 1 letter). The numbers should give information about the run and the sample. For instance, in the file name L002-026_R1.fastq.gz, it means that you are processing the project L, the run # 2 and the sample 026. Moreover, the suffix _R1 or _R2 designate forward or reverse reads. The MiSeq sequencer normally adds an extra suffix _0001 to the names of the fastq files L002-026_R1_0001.fastq.gz. These suffix is no problem for the pipeline to work. This pipeline needs a metadata (Excel file) should have a very specific format. For details about how to make the metadata file and what the codes mean, please consult with the microbiome team at the RIVM, especially Susana Fuentes. For the use of the pipeline, the main features that need to be added in this Excel file are: It should have the file extension .xlsx file (no .csv). It should have at least a sheet called amplicon_assay. The metadata file for the Triumph project includes many other sheets but this is the only one that is necessary. Leaving the sheet unnamed will not work even if the information inside the sheet is correctly formatted. The amplicon_assay sheet should have at least three columns with the names: assay_sample, sample_indentifier and subject_identifier. The assay_sample column should contain the root of the sample names that are used to name the fastq files (see previous point). For instance, the assay_sample code for the file L002-026_R1_0001.fastq.gz is L002-026. The sample_identifier column should contain the identity of the samples. Here specific codes are needed for the control samples. For instance, every non-control sample is named something like S12345678 where S denotes that it is a sample belonging to a patient and the numbers are a unique ID for that sample. In contrast, the control samples dont have unique IDs. They are always named the same way in every run. The accepted codes for the control samples can be found in the Table 1. The subject_identifier column contains the same code than sample_identifier and it is used as a back-up. When the codes in sample_identifier are not found, they are looked for instead in the subject_identifier column. TABLE 11.1: Table 1. Accepted subject_identifier codes for metadata used in the Triumph project. subject_identifier subject_description ZMCD Zymo mock DNA ZMCB Zymo mock Bacteria AMCD ATCC mock DNA UMD UMCU low density mock DNA MSS Mixed (fecal/NP/OP) sample control MSD Mixed (fecal/NP/OP) sample control DNA BD Blank from DNA extraction BP Blank from PCR LSIC Library spike-in control 11.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The triumph_microbiome_run pipeline can be found in this link. 11.1.3 Start the analysis Open the terminal. Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/triumph_dada2_run_pipeline-master/ Exact name of downloaded folder might be slightly different depending on the version that was downloaded. Run the pipeline bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ --metadata /path/to/metadata.xlsx Please read the section What to expect while running a Juno pipeline See the section General Troubleshooting for any problems you may encounter. Note: Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast. 11.1.3.1 Run the pipeline changing default parameters Many of the parameters of the dada2 package can be changed/used directly when calling the pipeline. If you want to do that, please refer to the help of the pipeline by using the command: bash triumph_run_pipeline.sh -h It should print something like this: Usage: bash triumph_run_pipeline.sh -i &lt;INPUT_DIR&gt; &lt;parameters&gt; Input: -i, --input [DIR] This is the folder containing the input (demultiplexed) fastq files. This should be a folder for one single run (sub-folder name will be considered the run name! so give it a meaningul one). Default is the current folder. --metadata [xlsx file] (optional) This is an optional parameter in which you provide an xlsx file with the metadata. For the exact format of this file, you should refer to the microbiome group at the RIVM (Susana Fuentes). In summary, it should contain a sheet called &#39;amplicon_assay&#39;, with an empty row at the top (it does not need to be empty, but that row will be ignored) and then a table with at least two columns: assay_sample and sample_identifier. The assay_sample should have the names coinciding with the fastq files (so, like the sample sheet used for demultiplexing) and the sample_identifier should contain either the sample code or one of the codes for recognizing control samples (ask the microbiome group). Main Output (automatically generated): out/ Contains detailed intermediate files. out/log/ Contains all log files. out/run_reports/ Folder containint the actual reports of the run (html file) and the summary table of the run (used to generate that report) as a csv file. Parameters: -o, --output Output directory. Default is &#39;out/&#39; --pienter-up If this option is chosen, the default options for the nasopharynx Triumph project will be used, which means that truncLenF will be overwritten to 200 and the truncLenR will be overwritten to 150. -h, --help Print this help document. -sh, --snakemake-help Print the Snakemake this help document. --clean (-y) Removes output. (-y forces &#39;Yes&#39; on all prompts) -u, --unlock Removes the lock on the working directory. This happens when a run ends abruptly and prevents you from doing subsequent analyses. -n, --dry_run Shows the process but does not actually run the pipeline. Dada2 parameters that can be changed in this pipeline (if you decide to be able to directly modify another parameter when you call the pipeline, contact me alejandra.hernandez.segura@rivm.nl) for more details: --trunQ Minimumn quality for truncating a read (part of dada2 filtering). Default: 2 --truncLenF Truncating length for forward reads(part of dada2 filtering). Default: 220 --truncLenR Truncating length for reverse reads(part of dada2 filtering). Default: 100 --maxN Maximum number of N bases accepted per read (part of dada2 filtering). Default: 0. Note that the dada2 requires this parameter to be 0. --maxEE Maximum Expected Errors (part of dada2 filtering). Default: 2 --nomatchID If this file is present, the reads will not be expected to match. The default is to expect filtered reads to have matching IDs (part of dada2 filtering). Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --nbases Number of bases to use in order to learn the errors (part of dada2 error learning). Default: 1e9 --minlen Minimum length of the inferred SAVs. Any SAV shorter than minlen will be discarded (part of dada2 inferring SAVs). Default: 250 --maxlen Maximum length of the inferred SAVs. Any SAV longer than maxlen will be discarded (part of dada2 inferring SAVs). Default: 256 --trainset Path to trainset used to assign taxonomy to a phyloseq object. Dada2 suggests a database in its tutorial. In the RIVM, we have a copy stored at: &#39;/mnt/db/triumph_taxonomy_db/silva_nr_v138_train_set.fa.gz&#39; and this path is the default. --species_db Path to database used to assign species name to a phyloseq object. Dada2 suggests a database in its tutorial. In the RIVM, we have a copy stored at: &#39;/mnt/db/triumph_taxonomy_db/silva_species_assignment_v138.fa.gz&#39; and this path is the default. --no-randomize-errors, -nr If this flag is present, there will not be randomiza- tion while learning the errors (part of dada2 learn errors). The default is to randomize. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --just-concatenate If this flag is present, the pairs will be simply concatenated. (part of dada2 mergePairs). The default is set to FALSE. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE. --bimera_method Choose a method to remove bimeras (according to dada2 removeBimeraDenovo). --minboot The minimum bootstrap confidence for assigning a taxonomic level (part of dada2 assignTaxonomy). Default: 80 --no-tryrc If TRUE, the reverse-complement of each sequences will be used for classification if it is a better match to the reference sequences than the forward sequence. The default is TRUE. Note that you should NOT put &#39;true&#39; or &#39;false&#39; after the flag. The flag itself forces the condition to be FALSE --species-allowed This is the number of species that may be enlisted (part of dada2 addSpecies). Default is 3. 11.1.3.2 Running when metadata is not available You can also run the pipeline without the metadata: bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ However, the report produced of such a run is not very informative and the code is not actively maintained. Therefore, the report might fail. The pipeline is not intended to be run without metadata so it will do its job but not efficiently. 11.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. If you chose another site for the output, then you will find the same results there. This folder will contain all the results and logging files of your analysis. There will be one folder with all the filtered_fastq files, one with the plots and one with the RDS_files generated when using the dada2 package (for instance, seqtab, merged files, etc.). These RDS files can be loaded into R where you can explore them and further work with them. The last folder is the run_reports which will have the html report with statistics of the run and the Project_SummaryTable.csv with all the statistics in a table format. Please refer to the dada2 documentation to interpret your results. Note: If you want your output to be stored in a folder with a different name or location, you can use the option -o (o from output) bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_last_run&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ Another very important output from the pipeline are the logging files and audit trail that contain information of the software versions used, the parameters used, the error messages, etc. They could be important for you if you want to publish or reproduce the analysis at a later timepoint and also to get help from the bioinformatics team if you were to run into trouble with the pipeline. Please read about these files here. 11.1.5 Troubleshooting Please read first the General Troubleshooting section! 11.1.5.1 Other problems or failing rules The triumph_microbiome_run pipeline is currently not maintained or supported. "],["triumph-project.html", "12 Triumph: Project report 12.1 Handbook", " 12 Triumph: Project report CURRENTLY NOT MAINTAINED OR SUPPORTED body { text-align: justify} The code here generates a report that collects the results of multiple runs of the Triumph report. Note that in order to use it, all the individual runs should have been analyzed with the Triumph: Run pipeline 12.1 Handbook 12.1.1 Requirements and preparation See the General Instructions for all pipelines first. Make sure that your folder has the right name and structure. Your input folder should have one subfolder per different run. Each of this subfolders contains the results of the Triumph: Run pipeline for that specific run. Please, do not rename any of the files produced by the Triumph: Run pipeline, otherwise they will not be found and cannot be included in the project report. Neither your folder nor your subfolders can have rare characters (only letters, numbers, dashes and underscores accepted). If your folder name contains different characters, it may not be recognized by the pipeline. IMPORTANT: Your folder name MUST NOT CONTAIN SPACES!!!! 12.1.2 Download the pipeline YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT Make sure to have followed the instructions to set up conda before installing any of our pipelines! Please follow the instructions to download pipelines from the Juno team of the IDS-bioinformatics group. The triumph_microbiome_project pipeline can be found in this link. 12.1.3 Start the analysis. Basics Open the terminal. Enter the folder of the pipeline: cd /mnt/scratch_dir/&lt;my_folder&gt;/triumph_project_report-master/ Exact name of downloaded folder might be slightly different depending on the version that was downloaded. Run the pipeline bash create_triumph_report.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/ The input directory (in this case /mnt/scratch_dir/my_folder) should be a folder containing one subfolder per run you want to include in the project report. These subfolders should contain the results of running the Triumph run pipeline using the --metadata option. Refer to the manual of that pipeline to see what that argument means. Note 1: If you are updating the project report because you added the analysis of an old run, you should first delete or rename the old report. This script might fail otherwise. Note 2: THE PROJECT REPORT PIPELINE ONLY WORKS WHEN ALL RUNS COLLECTED WERE EVALUATED WITH THE Triumph: Run pipeline USING A METADATA FILE!!! 12.1.4 Output A folder called out/, inside the folder of the pipeline, will be created. If you prefer to store your output in a different folder, you can use the -o argument to give another path. bash triumph_run_pipeline.sh -i /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_data&gt;/ -o /mnt/scratch_dir/&lt;my_folder&gt;/&lt;my_results&gt;/ If you chose another site for the output, then you will find the same results there. In both cases, the output folder will contain the Triumph_Project_report.html file that is the actual report and a Triumph_Project_Report.log that contains messages, warnings and errors from R, where you can find more info if something goes wrong and the report could not be produced. 12.1.5 Troubleshooting Please read first the General Troubleshooting section! 12.1.5.1 Other problems or failing rules The triumph_microbiome_project pipeline is currently not maintained or supported. "]]
