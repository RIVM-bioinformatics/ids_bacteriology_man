---
output:
  pdf_document: default
  html_document: default
---
# Triumph: Run pipeline {#triumph_run}

<style>
body {
text-align: justify}
</style>

```{r setup_triumph_run_report, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

This pipeline processes raw fastq files from microbiome data using the [Dada2](https://benjjneb.github.io/dada2/tutorial.html) R-package. At the end, it also produces a summary of the run. 

The pipeline has been tailor-made for the microbiome group at the RIVM, especifically for the Pienter project. It therefore expects the input that is normally generated there and might not be very flexible with sample names and metadata. I will try to explain what is expected from the input when necessary.

## Handbook

**Important note:** this handbook is in continuous development, so please keep that in mind and contact [Alejandra Hernandez](alejandra.hernandez.segura@rivm.nl) if you wish to add/change something or if you need help with troubleshooting (please read the troubleshooting section first). 

### Requirements and preparation

This handbook assumes that you are working at the “bioinformatica” environment at the RIVM. It is possible to run the pipeline in other settings and even on your laptop but you need extra steps that will not be enlisted here. 

- Placing of the data: Your demultiplexed fastq files should all be placed in one single folder (no subfolders) in the BioGrid (`/data/BioGrid/<my_folder>/<my_data>/`) or on the scratch_dir folder (`/mnt/scratch_dir/<my_folder>/<my_data>/`). I strongly advice you to place it on the scratch_dir “partition” and to only copy it to the BioGrid after the analysis. The run will be faster and therefore you will block the cluster less time for everyone else.  

- Make sure that your folder has the right name: the folder that contains your data can have any name you want (however, the name of the folder will be considered the run name) provided that you only use letters, numbers or underscores. If your folder name contains different characters, it may not be recognized by the pipeline. IMPORTANT: Your folder name MUST NOT CONTAIN SPACES!!!!  

- Make sure that your files have the right format: they must have a fastq extension (.fastq, .fq, .fastq.gz or .fq.gz) and have specific names. The names should be only numbers (they can be preceded by 1 letter). The numbers should give information about the run and the sample. For instance, in the file name L002-026_R1.fastq.gz, it means that you are processing the sample 26 of the run # 2 of the "L" project. Moreover, the suffix '_R1' or '_R2' designate forward or reverse reads.

### Download the pipeline   

**THIS SECTION IS ONLY RUN THE FIRST TIME YOU USE THE PIPELINE OR EVERY TIME YOU HAVE TO UPDATE IT**  

The pipeline should be downloaded in the same partition that your data is (BioGrid or scratch_dir). You can download it in two ways:  

_Website_  

1. Go to the website: https://gitl01-int-p.rivm.nl/hernanda/triumph_dada2_run_pipeline.git (Note: for now you need an invitation to be able to see the code. Once the pipeline is a bit more advanced in development, any RIVM member will have access to it).  
2. Press the small white button with a cloud and a downwards arrow. In the dropdown menu, choose "Download zip".  
3. A zip file (triumph_dada2_run_pipeline-master.zip) will have likely be downloaded on your “Downloads” folder. Please move this zip file to the BioGrid or the scratch_dir partitions, depending on where your data is.  
4. Extract the files of the zip file. In Linux this is normally done by pressing the left button of the mouse, then “Open with Archive Manager” and then press “Extract” on the two windows that will consecutively appear. You could then delete the zip file.  


_Command-line_   

Any member of the RIVM has a (RIVM-specific) GitLab account. You can log in with the same creadentials that you use for accessing your workspaces.    

1. Open the “terminal” (you could also open “terminator”) by going to the “Applications” menu of the linux environment.  
2. Go to the location where you want to download the pipeline using the command ‘cd’. For instance:  

```{bash}
cd /mnt/scratch_dir/<my_folder>/
```
*Note: mind the slash at the beginning of the path

3. Download the pipeline using the `git clone` command  

```{bash}

git clone https://gitl01-int-p.rivm.nl/hernanda/triumph_dada2_run_pipeline.git

```
 You will be asked to give your credentials and then the (already unzipped) pipeline should have been downloaded in your current folder.

### Start the analysis. Basics

Once you have the pipeline downloaded and placed in the right partition, you can start the analysis.  

1. Open the terminal. You can go to the Linux menu called "Applications" and open the program "terminal" or the "terminator" one.   
2. Enter the folder of the pipeline  

```{bash}
cd /mnt/scratch_dir/<my_folder>/triumph_dada2_run_pipeline-master/
```
* Exact name of downloaded folder might be slightly different depending on the version that was downloaded (e.g. -master might be something else).

3. Run the pipeline  

```{bash}
bash triumph_run_pipeline.sh -i /mnt/scratch_dir/<my_folder>/<my_data>/
```

The pipeline should start running by itself and give you instructions. The first time you run the pipeline, the preparation might take longer than expected and it will need more input from you. Please read the instructions and answer when prompted. 

The first thing that may happen is that you may get a message saying that:

```
The master environment hasn't been installed yet, do you want to install this environment now? [y/n]
```

You just have to answer with a `y` as prompted. The installation will start by itself. If the installation takes very long (let's say around 30 min and it is still in a message saying "Solving environment..."), refer to the Troubleshooting section expecially to the part "Failure installing master environment".  

In the next steps, many messages will appear on your screen. For instance, a run sheet will be generated, enlisting all the fastq files to be processed. Finally, the snakemake workflow (the actual pipeline) will run. Every time you download the pipeline again, many things have to be installed. Be patient! Especially, after snakemake has started, it will need to create many environments and this step might take some time. Please check regularly that the pipeline is running. Every time you will get yellow or green messages that tell you what is happening and that indicate that the pipeline is working as expected. You will see that every step or "rule" is being performed and a job is sent to the cluster. Snakemake will try to optimize the time by performing many steps simultaneously and inform you of the success/failure of every step. The failures will appear in red, but the pipeline might keep going to finsih all the steps that are possible. It regularly tells you how far it is (it gives a % of the steps that have been finished). 

Once your pipeline is finished you should see some final messages informing you that 100% of the steps were finished and that a report was created. You will find your results inside a folder called `out/` inside the folder of the pipeline. For instance:  `/mnt/scratch_dir/<my_folder>/triumph_dada2_run_pipeline/out`. You can then move the data to any location you prefer. **Note:** You can also choose another output directory by specifying it when calling the pipeline:

```{bash}
# Default out/ folder
bash triumph_run_pipeline.sh -i /mnt/scratch_dir/<my_folder>/<my_data>/

# Custum output folder
bash triumph_run_pipeline.sh -i /mnt/scratch_dir/<my_folder>/<my_data>/ -o /mnt/scratch_dir/<my_folder>/<my_results>/
```

See the section Troubleshooting for any problems you may encounter. 

**Note:** Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast.

#### Running when metadata is available

Since for now the metadata is only complete for few runs, the default behavior is to assume there is no metadata available. However, if you have metadata available in the form agreed with the Triumph project, it can be added to the report. The agreed format is an ".xlsx" file including a sheet called "amplicon_assay" that contains a table with at least the columns "assay_sample" and "sample_identifier", the latter using the agreed codes for control samples. IMPORTANT: the name of your metadata file can only contain numbers, letters, dashes and underscores. NO OHTER SPECIAL SYMBOLS AND NO SPACES ALLOWED!!

You can then call the pipeline like this:

```{bash}
bash triumph_run_pipeline.sh -i /mnt/scratch_dir/<my_folder>/200915_Run10_GutTriumph/ --metadata /mnt/scratch_dir/<my_folder>/200915_Run10_GutTriumph/my_metadata.xlsx
```

I highly recommend running the pipeline with metadata because the resulting report will be much more efficient. Moreover, it is necessary if you want to use the code for the Triumph Project Report (see next chapter).

### Output 

A folder called `out/`, inside the folder of the pipeline, will be created. If you chose another site for the output, then you will find the same results there. This folder will contain all the results and logging files of your analysis. There will be one folder with all the `filtered_fastq files`, one with the `plots` and one with the `RDS_files` generated when using the dada2 pipeline and one folder `run_reports` will have the html report with statistics of the run and the Project_SummaryTable.csv with all the statistics in a table format. There are also two important subfolders generated by this pipeline:

- The `out/log` folder contains a lot of information, including the snakemake_report.html that has a summary of all the analyses performed, the output generated and the software used. It also contains three sub-folders:  
    1. The out/log/<project>/ folder contains information about every step performed for every sample. There you can find error messages or some information of what happened during each step. The messages are not always easy to interpret, but they often have clues on why a job/analysis failed. Sometimes the log files for each tool (and sample) are empty because either, there were no problems or messages generated on the run or because the problem lies before the job/analysis was even started. In the later case, you may want to look at the subfolder `out/log/drmaa/`.  
    2. The `out/log/drmaa/` subfolder contains logging files of any job performed by the pipeline. Here it is not always easy to find the job you are looking for, but do know that they are there and they can be accessed if necessary. Note: if you chose a different output directory, the drmaa files will not be stored there. They will always be stored in `out/log/drmaa/` 
    3. The `out/log/parameters/` subfolder contains 4 very important files for traceability of your samples. The `log_conda.txt` file contains information about the software that was necessary and that was contained in your environment. This means basically the software that would be needed to reproduce the same circumstances in which the pipeline was run and how it can be reproduced. The `log_config.txt` file is even more informative. It enlists all the parameters used to run the pipeline. In case months laters you forgot how you got the results you did or you just want to know some details about the analyses, they are all stored there.  

- The `out/benchmark` folder contains information about the time and computer resources used for each step of the pipeline.  

### Troubleshooting 

#### Failure installing master environment

Sometimes the installation does not work because of the settings of Linux. The installation always takes some minutes in which a message saying `Solving environment...` might appear on the screen. However, if this step takes too long (for instance 30 min or more) please abort the step by pressing `Ctrl + C` and/or `Ctrl + Z` and change the necessary settings: 

```{bash}
conda config --set channel_priority false
```

The try to run the pipeline and install the master environment again.  

#### Failure to make a run sheet or to find input directory

If this step fails and you get a message like: 

```
The input directory you specified INPUT_DIR exists but is empty or does not contain the expected input files...  
Please specify a directory with input-data.
```

This message tells you that your folder does not exist. Please check that you put the right path to the folder with your data (`/mnt/scratch_dir/<my_folder>/<my_data>/`) when running the pipeline. Check that there are no typos or that you did not put the wrong name of your input folder. 

```
Couldn't find files in the input directory that ended up being in a .FASTQ, .FQ or .GZ format.  
Please inspect the input directory INPUT_DIRECTORY and make sure the files are in one of the formats listed below:  
    .fastq.gz (Zipped Fastq)  
    .fq.gz (Zipped Fq)  
    .fastq (Unzipped Fastq)  
    .fq (unzipped Fq) 
```
    
This message means that your directory exists but the files do not have the name that is expected. Please refer to requirements to make sure that your files and folder have the right name (right extension and right letters on the name).

#### Other problems or failing rules

Snakemake and especially this pipeline are not perfect. Sometimes things fail. Before contacting me for help, please try to re-run the pipeline again and see if it goes further and the rules or the whole analysis can be completed or at least go further than before. If it does, please repeat that step (re-running the pipeline) until your analyis is finished and just inform me via e-mail so I can try to find the error later. If the pipeline does not go further or you get strange messages, try to DOWNLOAD THE PIPELINE AGAIN (so, start following this handbook from the beginning) just in case this issue was noted before and already solved in a new version of the pipeline. If nothing helps, please contact me and I will try to help you troubleshooting. 

**BEFORE CONTACTING ME READ THIS:**
If you need to contact me, please attach to the email a copy the messages that appeared in your screen and look for the logging file (in the folder `out/log/`) that belongs to the step and the sample that failed (or at least one of them if there were multiple). If you cannot find the file or it is empty, look for the newest files with the extension `.out` or `.err` in the `out/log/drmaa/` folder. These files often contain error messages as well. Send the files to me or at least keep them at hand. Also, do not forget to mention where your code and data are located so I can have a look at them (if I have access).