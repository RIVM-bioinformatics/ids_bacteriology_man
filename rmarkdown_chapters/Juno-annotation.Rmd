# Juno-annotation {#juno-annotation}

<style>
body {
text-align: justify}
</style>


```{r, echo = FALSE, eval = TRUE}
# Parameters
parameters <- list("pipeline_name" = "Juno-annotation")

# Load necessary libraries
library(dplyr)
library(tidyr)

```

This pipeline takes assemblies (.fasta) as input and performs gene annotation. It should be able to annotate bacterial genomes and bacterial plasmids. The pipeline follows these steps:

1. Filtering contigs with more than 200bp. This step is necessary for downstream tools to run properly and, in general, small contigs are often filtered to reduce the noise of possible contamination. Besides, these small contigs often do not contain much valuable information.  

2. Finding the start of the chromosome ( [Circlator](https://github.com/sanger-pathogens/circlator) ). Although in most cases, the chromosome is already circularized after the assembly, in the cases in which that was not possible, this step might help. The start of the chromosome will be set at the beginning of the _dnaA_ gene. For the plasmids or other smaller contigs, the start of a predicted gene near its center is used.  

3. Annotation using [Prokka](https://github.com/tseemann/prokka). Prokka is a fast program for prokaryotic genome annotation. We enriched the databases that Prokka relies on by using the RefSeq database. This step is very slow if run locally (~ 3 hours) but in the cluster it takes around 30 min per sample.   

## Handbook

### Requirements and preparation

See the [General Instructions for all pipelines](#general-instructions) first.  

- The pipeline needs to know the "Genus" and "Species" of each input fasta file. There are three ways to do this (see the section Run the pipeline for more details on how to do this and the recognized abbreviations): 
    + The pipeline can guess this information from the file name provided the appropriate abbreviations are used within the name. 
    + You can provide a metadata file (.csv) that should contain at least three columns: "File_name", "Genus" and "Species" (mind the capital letters). The File name is case sensitive, so the names of the files (without the full path) should coincide EXACTLY with your input fasta files. The genus and species should be recognized as an official [TaxID](https://www.ncbi.nlm.nih.gov/taxonomy). You MUST write the genus and the species in the appropriate column, never together in one column. This option cannot be combined with the first one. This means that if you decide to "guess" the genus and species from the file name, the provided metadata file will be ignored.
    + You can provide the genus and species directly when calling the pipeline. The genus and species should be recognized as an official [TaxID](https://www.ncbi.nlm.nih.gov/taxonomy). YOU CAN ONLY PROVIDE ONE GENUS AND ONE SPECIES and this will be used for all samples. You can combine this option with one of the two above. This means that if you have multiple samples but some of them are not enlisted in your metadata file, they will instead inherit the genus and species from the information provided directly when calling the pipeline. If all the files were enlisted in the metadata, this option will be ignored.

### Download the pipeline  

**YOU NEED TO DOWNLOAD THE PIPELINE ONCE OR EVERY TIME YOU WANT TO UPDATE IT**

Please follow the [instructions to download pipelines](#downloading-instructions) from the Juno team of the IDS-bioinformatics group. The `r parameters$pipeline_name` pipeline can be found in [this link](https://github.com/RIVM-bioinformatics/Juno-annotation).  


### Start the analysis. Basics

1. Open a terminal. (Applications>terminal).  
2. Enter the folder of the pipeline using:  

```{bash}

cd /mnt/scratch_dir/<my_folder>/Juno-annotation

```

3. Run the pipeline  

If all your samples have the same genus and species, for instance, _Klebsiella pneumoniae_, you run it like this:

```{bash}
bash start_annotation.sh -i /mnt/scratch_dir/<my_folder>/<my_data>/ --genus Klebsiella --species pneumoniae
```
* Note that the genus and species should be ONE word. Do not put genus and species together!

If your samples do not have all the same genus, you can make a metadata file. This file should be a .csv file with at least these columns and information: 

```{r eval=TRUE, echo=FALSE}

fake_metadata <- data.frame("File_name" = c("sample1_Kpn.fasta", "sample2Pae2.fasta", "sample3Sau_1.fasta"),
                            "Genus" = c("Klebsiella", "Pseudomonas", "Staphylococcus"),
                            "Species" = c("pneumoniae", "aeruginosa", "aureus"))

knitr::kable(fake_metadata)

```
* Note that the name of the columns should be EXACTLY the same than in this example, including the underscores and the capital letters.

You would then call the pipeline like this:

```{bash}
bash start_annotation.sh -i /mnt/scratch_dir/<my_folder>/<my_data>/ --metadata path/to/metadata.csv
```

Alternatively, if your input files have one of the following abbreviations somewhere in the data, the genus and species may be automatically guessed from the file name:

```{r eval=TRUE, echo=FALSE}

download.file("https://raw.github.com/AleSR13/AMR_annotation/master/files/dictionary_samples.yaml", destfile = "files/Annotation_pipeline_translation_table.yaml")

translation_table <- yaml::read_yaml("files/Annotation_pipeline_translation_table.yaml") %>%
  as.data.frame() %>%
  pivot_longer(cols = colnames(.), values_to = "Values", names_to = "Abbreviation") %>%
  mutate("GenSp" = ifelse(stringr::str_detect(.$Abbreviation, "Species"), "Species", "Genus"),
         "Abbreviation" = stringr::str_remove(Abbreviation, ".Species|.Genus")) %>%
  pivot_wider(id_cols = "Abbreviation", names_from = "GenSp", values_from = "Values")

knitr::kable(translation_table)

```

Then you would call the pipeline like this:

```{bash}
bash start_annotation.sh -i /mnt/scratch_dir/<my_folder>/<my_data>/ --make-metadata
```

Please read the section [What to expect while running a Juno pipeline](#what-to-expect)

See the section [General Troubleshooting](#general-troubleshooting) for any problems you may encounter. 

**Note:** Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast.

### Output 

A folder called `out/`, inside the folder of the pipeline, will be created. This folder will contain all the results and logging files of your analysis. There will be one folder per tool (`circlator`, `filtered_contigs` and `prokka`). Please refer to the manuals of every tool to interpret the results. Your **main result** will be two genebank (.gbk) files per sample that contained your annotated genomes/plasmids. You can find them inside the subfolders `out/prokka`. 

*Note:* If you want your output to be stored in a folder with a different name or location, you can use the option `-o` (from output) 

```{bash}
bash start_annotation.sh -i /mnt/scratch_dir/<my_folder>/<my_data>/ -o /mnt/scratch_dir/<my_folder>/<my_results>/
```

There are also two important subfolders generated by this pipeline:

- The `out/log/` folder contains information about every step performed for every sample. There you can find error messages or some information of what happened during each step. The messages are not always easy to interpret, but they often have clues on why a job/analysis failed. Sometimes the log files for each tool (and sample) are empty because either, there were no problems or messages generated on the run or because the problem lies before the job/analysis was even started. In the later case, you may want to look at the subfolder `out/log/drmaa/` where you can find logging files of any job performed by the pipeline. Here it is not always easy to find the job you are looking for, but do know that they are there and they can be accessed if necessary.

- The `out/results/` folder contains 4 very important files for traceability of your samples. The `log_conda.txt file` contains information about the software that was necessary and that was contained in your environment. This means basically the software that would be needed to reproduce the same circumstances in which the pipeline was run and how it can be reproduced. The `log_config.txt` file is even more informative. It enlists all the parameters used to run the pipeline. In case months laters you forgot how you got the results you did or you just want to know some details about the analyses, they are all stored there. The `log_git.txt` has information about the repository or the code that was downloaded. It tells you exactly how it was donwloaded so you can reproduce it at a later timepoint. Finally, the `snakemake_report.html` has a nice overview of the different steps that were performed with your samples, when were they performed, which output was produced and which software was used, as well as some statistics on how the run went. 

### Troubleshooting for this pipeline

**Please read first the [General Troubleshooting](#general-troubleshooting) section!**

#### Failure to find metadata file

You could get an error associated to the metadata:

```
The provided species file <your_metadata_file.csv> does not exist. Please provide an existing file
"If you used the option --make-metadata, please check that all the fasta files contain the .fasta extension and that the file names have the right abbreviations for genus/species
```
    
This message means that either you provided the wrong path/name to your metadata file, that this has the wrong extension (not .csv) or that, if you used the option to `--make-metadata`, your input files do not have the right abbreviations. Please check the metadata file provided or the file names and try again.

#### Other problems or failing rules  

The `r parameters$pipeline_name` pipeline is still in development which means that sometimes things fail. Before contacting me for help, please try these steps:  

1. Re-run the pipeline again and see if it goes further. If it does, please keep re-running the pipeline until your analyis is finished or it just doesn't go further. Even if you are able to finish your analysis, just send me an email afterwards (see step 3) so I can check what happened.    
2. Download the pipeline again and start from the beginning of this handbook. Sometimes the issue has been resolved in newer versions of the pipeline.
3. Collect your logging files and contact me. Please inform me about bugs/errors via [e-mail](alejandra.hernandez.segura@rivm.nl) **sending also your `log` files and the path where I can find your input directory and the pipeline**. No screenshots are necessary. Note that if you do not send this information, I will not be able to help you and your and my work will be delayed.  
