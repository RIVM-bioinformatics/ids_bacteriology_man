---
title: "Pipelines IDS-BPD Bioinformatics"
author: "Alejandra Hernandez-Segura"
date: "`r Sys.Date()`"
output: pdf_document
biblio-style: apalike
link-citations: yes
highlight: tango
documentclass: book
---

```{r}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE,
                      error = FALSE,
                      message = FALSE,
                      warning = FALSE)
```

# Introduction and General Instructions

In this "book" you will find the handbooks of all the available pipelines for the Bacteriology and Parasitology Department of the IDS (RIVM). The handbooks are updated regularly but if you would find a mistake or something that is not up-to-date, please communicate with:

- Alejandra Hernandez ( [email](mailto:alejandra.hernandez.segura@rivm.nl) )

.extralarge[**Important note:**]
This handbook is in continuous development, so please keep that in mind and contact [Alejandra Hernandez](mailto:alejandra.hernandez.segura@rivm.nl) if you wish to add/change something or if you need help with troubleshooting (please read the troubleshooting section for each pipeline first). 

## General Instructions for all pipelines {#general-instructions}

### Requirements and preparation

This handbook assumes that you are working at the “bioinformatica” environment at the RIVM. It is possible to run the pipeline in other settings and even on your laptop but you need extra steps that will not be enlisted here.  

- Placing of the data: Your data should all be placed in one single folder (no subfolders) in the BioGrid (`/data/BioGrid/<my_folder>/<my_data>/`) or in the scratch_dir folder (`/mnt/scratch_dir/<my_folder>/<my_data>/`). I strongly advice you to place it in the `scratch_dir` folder and to only copy it later to the BioGrid after the analysis. The run will be faster and therefore you will block the cluster less time for everyone else.  

- Make sure that the folder that you use as input (it contains your input data) has the right name. This means: the folder that contains your data can have any name you want provided that you only use letters, numbers or underscores. If your folder name contains different characters, it may not be recognized by the pipeline. **IMPORTANT:** Your folder name and the PATH to it (so all the folders and subfolders that you have to enter to reach your folder) should NOT CONTAIN SPACES!!!!  

- Make sure that your files have the right format: if they are fastq files, they should have the extension (`.fastq`, `.fq`, `.fastq.gz` or `.fq.gz`). If they are fasta files, they should have the extension `.fasta`. Any other requirements in the input files will be specified in the section of that specific pipeline.  

### Downloading pipelines {#downloading-instructions}

All the bacteriology pipelines created by the IDS-bioinformatics group are stored in either [GitHub](https://github.com/RIVM-bioinformatics) or the [internal GitLab](https://gitl01-int-p.rivm.nl/) of the RIVM. Only people who belong to the RIVM and that are inside one of our servers/environments can access to the later one with their normal RIVM login details.  

If you are going to download a pipeline, please do so in the same partition that your data is (preferentially `scratch_dir`). Each pipeline handbook has the instructions on where to find the code (either GitHub or GitLab). You can download every pipeline through the website or through the command line:  

_GitHub website_  

1. Go to the website for the pipeline (check the section of the specific pipeline).  
2. Press the green button “Code” on the page and then click on “Download zip” (see [Figure](#github-fig) for explanation).  
3. A zip file (<pipeline_name>-master.zip) will have likely be downloaded on your “Downloads” folder. Please move this zip file to the `BioGrid` or the `scratch_dir` partitions, depending on where your data is.  
4. Extract the files of the zip file. In Linux this is normally done by pressing the left button of the mouse, then “Open with Archive Manager” and then press “Extract” on the two windows that will consecutively appear. You could then delete the zip file (see [Figure](#extract-fig) for explanation).   

```{r github-fig echo = FALSE, eval = TRUE, fig.cap = "Pipelines can be downloaded directly from their GitHub website."}
knitr::include_graphics("figures/screenshot_download_github.png")
```
```{r extract-fig echo = FALSE, eval = TRUE, fig.cap = "Unzipping a repository in Linux."}
knitr::include_graphics("figures/screenshot_unzip.png")
```

_GitLab website_  

1. Go to the website for the pipeline (check the section of the specific pipeline).   
2. Press the small white button with a cloud and a downwards arrow. In the drop-down menu, choose "Download zip" (see [Figure](#gitlab-fig) for explanation).   
3. A zip file (<pipeline_name>-master.zip) will have likely be downloaded on your “Downloads” folder. Please move this zip file to the `BioGrid` or the `scratch_dir` partitions, depending on where your data is.  
4. Extract the files of the zip file. In Linux this is normally done by pressing the left button of the mouse, then “Open with Archive Manager” and then press “Extract” on the two windows that will consecutively appear. You could then delete the zip file (see [Figure](#extract-fig) above for explanation).  

```{r gitlab-fig echo = FALSE, eval = TRUE, fig.cap = "Pipelines can be downloaded directly from their GitLab website."}
knitr::include_graphics("figures/screenshot_download_gitlab.png")
```

_Command-line_   

Any member of the RIVM has a (RIVM-specific) GitLab account. You can log in with the same credentials that you use for accessing your workspaces. In the case of pipelines hosted on GitHub, you may need a free GitHub account.     

1. Open the “terminal” (you could also open “terminator”) by going to the “Applications” menu of the linux environment.  

```{r gitlab-fig echo = FALSE, eval = TRUE, fig.cap = "Pipelines can be downloaded directly from their GitLab website."}
knitr::include_graphics("figures/screenshot_terminal.png")
```

2. Go to the location where you want to download the pipeline using the command ‘cd’. For instance:  

```{bash, eval = FALSE}
cd /mnt/scratch_dir/<my_folder>/
```
**Note:** mind the slash at the beginning of the path

3. Download the pipeline using the `git clone` command  

```{bash, eval = FALSE}

git clone <url_to_the_pipeline>.git

```
**Note**: notice that I added ".git" to the URL of the pipeline.  

You will be asked to give your credentials (username + password) and then the (already unzipped) pipeline should have been downloaded in your current folder. 

### What to expect while running a Juno pipeline {#what-to-expect}  

Our pipelines normally run by themselves. Sometimes they ask for input from the user to agree on installing software or a database but other than that, they do not require much input from the user.Try to read at least the last lines appearing on the screen and if you get asked to give a permission, please do. The first time you run a new pipeline, the preparation might take longer than expected. This is due to the installation of the basic software needed for it to work. Be patient! The installation may take some time.  

Once the installation is over, the pipeline should start running. Please check regularly that the pipeline is actually running and working. Every time you will get <span style=”color:yellow;”>yellow</span> or <span style=”color:green;”>green</span> messages that tell you what is happening and that indicate that the pipeline is working as expected. Multiple steps are performed simultaneously to optimize the time and resources, especially when using the cluster. If a step fails, you will likely see some <span style=”color:red;”>red</span> text appearing on the screen, but the pipeline might keep going to finsih all the steps that are possible. The messages on the screen regularly tell you how far the pipeline is (it gives a % of the steps that have been finished). 

Once your pipeline is finished you should see some final messages informing you that 100% of the steps were finished and that your output was created. 

See the section [General Troubleshooting](#general-troubleshooting) for any problems you may encounter. 

**Note:** Do not keep all your data (including results) on the scratch_dir partition. You are allowed to keep 400GB max and with sequencing data, this can get full quite fast.

### General output and log files for every pipeline {#general-output}  

Every pipeline (with few exceptions of the very short ones like the deinterleave one) generates some extra output that is not essential for your analysis but that it is required for debugging/troubleshooting. Especially, if you are contacting someone at the IDS-bioinformatics team, we expect you to have these files at hand or to also send the location where they can be found (make sure that we have right to access that location or move them to a shared folder where they can be accessed).  

- The `log/` subfolder is located inside your output directory. It contains one file per every step performed by the pipeline for each sample. There you can find error messages or some information of what happened during each step. The messages are not always easy to interpret, but they often have clues on why a job/analysis failed. Sometimes the log files for each tool (and sample) are empty because either, there were no problems or messages generated on the run or because the problem lies before the job/analysis was even started. You can look at them of course, but it is ok if you do not understand the messages. However, for us as bioinformaticians it is essential to have access to these files when something goes wrong inside one of the scripts/tools used.

- The `log/drmaa/` subfolder is also inside the output directory and inside the log subdirectory. Here you can find other type of logging files of any job performed by the pipeline. This is even more technical logging but it can be veru useful to debug system errors, problems with memory, etc.  

- The `audit_trail` or `results` subfolders are also located inside the output directory. They contain contain 4 very important files for trace-ability of your samples. The `log_conda.txt file` contains information about the software that was necessary and that was contained in your environment. This means basically the software that would be needed to reproduce the same circumstances in which the pipeline was run and how it can be reproduced. The `log_config.txt` file is even more informative. It enlists all the parameters used to run the pipeline. In case months later you forgot how you got the results you did or you just want to know some details about the analyses, they are all stored there. The `log_git.txt` has information about the repository or the code that was downloaded. It tells you exactly how it was downloaded so you can reproduce it at a later time point. Finally, the `snakemake_report.html` has a nice overview of the different steps that were performed with your samples, when were they performed, which output was produced and which software was used, as well as some statistics on how the run went. 

Even though you would not look at this output regularly, it is useful for us as bioinformaticians to be able to help you if a porblem would arise. However, <span style="font-weight:bold;">they are also useful for you!</span> Especially the `audit_trail` has the information you would need to put in a publication (software versions, parameters, etc.). <span style="font-weight:bold;">Please do not delete them</span> and know they are important.  